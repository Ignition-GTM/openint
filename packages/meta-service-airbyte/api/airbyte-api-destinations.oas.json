{"openapi":"3.1.0","info":{"title":"Destinations","version":"1.0.0","description":"Programatically control Airbyte Cloud, OSS & Enterprise."},"servers":[{"url":"https://api.airbyte.com/v1","description":"Airbyte API v1"}],"paths":{"/destinations":{"get":{"tags":["Destinations"],"responses":{"200":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/DestinationsResponse"}}},"description":"Successful operation"},"403":{"description":"Not allowed"},"404":{"description":"Not found"}},"operationId":"listDestinations","summary":"List destinations","x-use-speakeasy-middleware":true,"parameters":[{"name":"workspaceIds","description":"The UUIDs of the workspaces you wish to list destinations for. Empty list will retrieve all allowed workspaces.","schema":{"type":"array","items":{"format":"uuid","type":"string"}},"in":"query","required":false},{"name":"includeDeleted","description":"Include deleted destinations in the returned results.","schema":{"default":false,"type":"boolean"},"in":"query","required":false},{"name":"limit","description":"Set the limit on the number of destinations returned. The default is 20.","schema":{"format":"int32","type":"integer","minimum":1,"maximum":100,"default":20},"in":"query"},{"name":"offset","description":"Set the offset to start at when returning destinations. The default is 0","schema":{"type":"integer","format":"int32","minimum":0,"default":0},"in":"query"}]},"post":{"requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/DestinationCreateRequest"},"examples":{"Destination Creation Request Example":{"value":{"name":"Postgres","workspaceId":"2155ae5a-de39-4808-af6a-16fe7b8b4ed2","configuration":{"airbyte_destination_name":"postgres","port":5432,"schema":"public","ssl_mode":{"mode":"prefer"},"tunnel_method":{"tunnel_method":"NO_TUNNEL"},"host":"localhost","database":"postgres","username":"postgres","password":"test"}}}}}}},"tags":["Destinations"],"responses":{"200":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/DestinationResponse"},"examples":{"Destination Creation Response Example":{"value":{"destinationId":"af0c3c67-aa61-419f-8922-95b0bf840e86"}}}}},"description":"Successful operation"},"400":{"description":"Invalid data"},"403":{"description":"Not allowed"},"404":{"description":"Not found"}},"operationId":"createDestination","summary":"Create a destination","description":"Creates a destination given a name, workspace id, and a json blob containing the configuration for the source.","x-use-speakeasy-middleware":true}},"/destinations/{destinationId}":{"get":{"tags":["Destinations"],"responses":{"200":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/DestinationResponse"},"examples":{"Destination Get Response Example":{"value":{"destinationId":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826","name":"My Destination","sourceType":"postgres","workspaceId":"744cc0ed-7f05-4949-9e60-2a814f90c035","configuration":{"conversion_window_days":14,"customer_id":"1234567890","start_date":"2023-01-01","end_date":"2024-01-01"}}}}}},"description":"Get a Destination by the id in the path."},"403":{"description":"Not allowed"},"404":{"description":"Not found"}},"operationId":"getDestination","summary":"Get Destination details","x-use-speakeasy-middleware":true},"delete":{"tags":["Destinations"],"responses":{"204":{"description":"The resource was deleted successfully"},"403":{"description":"Not allowed"},"404":{"description":"Not found"}},"operationId":"deleteDestination","summary":"Delete a Destination","x-use-speakeasy-middleware":true},"patch":{"tags":["Destinations"],"requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/DestinationPatchRequest"},"examples":{"Destination Update Request Example":{"value":{"configuration":{"conversion_window_days":14,"customer_id":"1234567890","start_date":"2023-01-01","end_date":"2024-01-01"},"name":"My Destination"}}}}}},"responses":{"200":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/DestinationResponse"},"examples":{"Destination Update Response Example":{"value":{"destinationId":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826","name":"running","sourceType":"postgres","workspaceId":"744cc0ed-7f05-4949-9e60-2a814f90c035","configuration":{"conversion_window_days":14,"customer_id":"1234567890","start_date":"2023-01-01","end_date":"2024-01-01"}}}}}},"description":"Update a Destination"},"403":{"description":"Not allowed"},"404":{"description":"Not found"}},"operationId":"patchDestination","summary":"Update a Destination","x-use-speakeasy-middleware":true},"put":{"tags":["Destinations"],"requestBody":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/DestinationPutRequest"},"examples":{"Destination Update Request Example":{"value":{"configuration":{"conversion_window_days":14,"customer_id":"1234567890","start_date":"2023-01-01","end_date":"2024-01-01"},"name":"My Destination"}}}}}},"responses":{"200":{"content":{"application/json":{"schema":{"$ref":"#/components/schemas/DestinationResponse"},"examples":{"Destination Update Response Example":{"value":{"destinationId":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826","name":"running","sourceType":"postgres","workspaceId":"744cc0ed-7f05-4949-9e60-2a814f90c035","configuration":{"conversion_window_days":14,"customer_id":"1234567890","start_date":"2023-01-01","end_date":"2024-01-01"}}}}}},"description":"Update a Destination and fully overwrite it"},"403":{"description":"Not allowed"},"404":{"description":"Not found"}},"operationId":"putDestination","summary":"Update a Destination and fully overwrite it","x-use-speakeasy-middleware":true,"x-speakeasy-entity-operation":"Destination#update"},"parameters":[{"name":"destinationId","schema":{"format":"UUID","type":"string"},"in":"path","required":true}]}},"components":{"responses":{"InitiateOauthResponse":{"content":{"application/json":{}},"description":"Response from the initiate OAuth call should be an object with a single property which will be the `redirect_url`. If a user is redirected to this URL, they'll be prompted by the identity provider to authenticate."}},"schemas":{"RedirectUrlResponse":{"title":"Root Type for RedirectUrlResponse","description":"","type":"object","properties":{"redirectUrl":{"format":"url","type":"string"}},"example":{"redirectUrl":"https://airbyte.portal.speakeasyapi.dev?speakeasyAccessToken=eydas.ad45.1234"}},"JobResponse":{"title":"Root Type for JobResponse","description":"Provides details of a single job.","required":["jobId","status","jobType","startTime","connectionId"],"type":"object","properties":{"jobId":{"format":"int64","type":"integer"},"status":{"$ref":"#/components/schemas/JobStatusEnum"},"jobType":{"$ref":"#/components/schemas/JobTypeEnum"},"startTime":{"type":"string"},"connectionId":{"format":"UUID","type":"string"},"lastUpdatedAt":{"type":"string"},"duration":{"description":"Duration of a sync in ISO_8601 format","type":"string"},"bytesSynced":{"format":"int64","type":"integer"},"rowsSynced":{"format":"int64","type":"integer"}},"example":{"id":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826","status":"running","jobType":"sync","startTime":"2023-03-25T01:30:50Z","duration":"PT8H6M12S"}},"JobsResponse":{"title":"Root Type for JobsResponse","description":"","required":["data"],"type":"object","properties":{"previous":{"type":"string"},"next":{"type":"string"},"data":{"type":"array","items":{"$ref":"#/components/schemas/JobResponse"}}},"example":{"next":"https://api.airbyte.com/v1/jobs?limit=5&offset=10","previous":"https://api.airbyte.com/v1/jobs?limit=5&offset=0","data":[{"id":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826","status":"running","jobType":"sync","startTime":"2023-03-25T01:30:50Z"}]}},"ConnectionCreateRequest":{"required":["sourceId","destinationId"],"type":"object","properties":{"name":{"description":"Optional name of the connection","type":"string"},"sourceId":{"format":"uuid","type":"string"},"destinationId":{"format":"uuid","type":"string"},"configurations":{"$ref":"#/components/schemas/StreamConfigurations"},"schedule":{"$ref":"#/components/schemas/ConnectionSchedule"},"dataResidency":{"$ref":"#/components/schemas/GeographyEnum"},"namespaceDefinition":{"$ref":"#/components/schemas/NamespaceDefinitionEnum"},"namespaceFormat":{"type":"string","description":"Used when namespaceDefinition is 'custom_format'. If blank then behaves like namespaceDefinition = 'destination'. If \"${SOURCE_NAMESPACE}\" then behaves like namespaceDefinition = 'source'.","default":null,"example":"${SOURCE_NAMESPACE}"},"prefix":{"type":"string","description":"Prefix that will be prepended to the name of each stream when it is written to the destination (ex. “airbyte_” causes “projects” => “airbyte_projects”)."},"nonBreakingSchemaUpdatesBehavior":{"$ref":"#/components/schemas/NonBreakingSchemaUpdatesBehaviorEnum"},"status":{"$ref":"#/components/schemas/ConnectionStatusEnum"}},"x-speakeasy-entity":"Connection","x-speakeasy-param-suppress-computed-diff":true},"ConnectionPatchRequest":{"type":"object","properties":{"name":{"description":"Optional name of the connection","type":"string"},"configurations":{"$ref":"#/components/schemas/StreamConfigurations"},"schedule":{"$ref":"#/components/schemas/ConnectionSchedule"},"dataResidency":{"$ref":"#/components/schemas/GeographyEnumNoDefault"},"namespaceDefinition":{"$ref":"#/components/schemas/NamespaceDefinitionEnumNoDefault"},"namespaceFormat":{"type":"string","description":"Used when namespaceDefinition is 'custom_format'. If blank then behaves like namespaceDefinition = 'destination'. If \"${SOURCE_NAMESPACE}\" then behaves like namespaceDefinition = 'source'.","default":null,"example":"${SOURCE_NAMESPACE}"},"prefix":{"type":"string","description":"Prefix that will be prepended to the name of each stream when it is written to the destination (ex. “airbyte_” causes “projects” => “airbyte_projects”)."},"nonBreakingSchemaUpdatesBehavior":{"$ref":"#/components/schemas/NonBreakingSchemaUpdatesBehaviorEnumNoDefault"},"status":{"$ref":"#/components/schemas/ConnectionStatusEnum"}},"x-speakeasy-entity":"Connection","x-speakeasy-param-suppress-computed-diff":true},"JobCreateRequest":{"title":"Root Type for JobCreate","description":"Creates a new Job from the configuration provided in the request body.","required":["jobType","connectionId"],"type":"object","properties":{"connectionId":{"format":"UUID","type":"string"},"jobType":{"$ref":"#/components/schemas/JobTypeEnum"}},"example":{"connectionId":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826","jobType":"sync"}},"JobStatusEnum":{"enum":["pending","running","incomplete","failed","succeeded","cancelled"],"type":"string"},"JobTypeEnum":{"description":"Enum that describes the different types of jobs that the platform runs.","enum":["sync","reset"],"type":"string"},"SourceCreateRequest":{"required":["name","workspaceId","configuration"],"type":"object","properties":{"name":{"type":"string"},"workspaceId":{"format":"uuid","type":"string"},"configuration":{"$ref":"#/components/schemas/SourceConfiguration"},"secretId":{"description":"Optional secretID obtained through the public API OAuth redirect flow.","type":"string"}},"x-implements":"io.airbyte.public_api.server.helpers.ConfigurableActor","x-speakeasy-entity":"Source","x-speakeasy-param-suppress-computed-diff":true},"SourcePutRequest":{"required":["name","configuration"],"type":"object","properties":{"name":{"type":"string"},"configuration":{"$ref":"#/components/schemas/SourceConfiguration"}},"x-implements":"io.airbyte.public_api.server.helpers.ConfigurableActor","x-speakeasy-entity":"Source","x-speakeasy-param-suppress-computed-diff":true},"SourcePatchRequest":{"type":"object","properties":{"name":{"type":"string","example":"My source"},"workspaceId":{"format":"uuid","type":"string"},"configuration":{"$ref":"#/components/schemas/SourceConfiguration"},"secretId":{"description":"Optional secretID obtained through the public API OAuth redirect flow.","type":"string"}},"x-implements":"io.airbyte.public_api.server.helpers.ConfigurableActor","x-speakeasy-entity":"Source","x-speakeasy-param-suppress-computed-diff":true},"OAuthInputConfiguration":{"description":"Arbitrary vars to pass for OAuth depending on what the source/destination spec requires.","type":"object","example":{"host":"test.snowflake.com"}},"ConnectionResponse":{"title":"Root Type for ConnectionResponse","description":"Provides details of a single connection.","type":"object","required":["connectionId","name","sourceId","destinationId","workspaceId","status","schedule","dataResidency","configurations"],"properties":{"connectionId":{"format":"UUID","type":"string"},"name":{"type":"string"},"sourceId":{"format":"UUID","type":"string"},"destinationId":{"format":"UUID","type":"string"},"workspaceId":{"format":"UUID","type":"string"},"status":{"$ref":"#/components/schemas/ConnectionStatusEnum"},"schedule":{"$ref":"#/components/schemas/ConnectionScheduleResponse"},"dataResidency":{"$ref":"#/components/schemas/GeographyEnum"},"nonBreakingSchemaUpdatesBehavior":{"$ref":"#/components/schemas/NonBreakingSchemaUpdatesBehaviorEnum"},"namespaceDefinition":{"$ref":"#/components/schemas/NamespaceDefinitionEnum"},"namespaceFormat":{"type":"string"},"prefix":{"type":"string"},"configurations":{"$ref":"#/components/schemas/StreamConfigurations"}},"x-speakeasy-entity":"Connection","x-speakeasy-param-suppress-computed-diff":true},"ConnectionSchedule":{"description":"schedule for when the the connection should run, per the schedule type","type":"object","required":["scheduleType"],"properties":{"scheduleType":{"$ref":"#/components/schemas/ScheduleTypeEnum"},"cronExpression":{"type":"string"}}},"ScheduleTypeEnum":{"type":"string","enum":["manual","cron"]},"ConnectionScheduleResponse":{"description":"schedule for when the the connection should run, per the schedule type","type":"object","required":["scheduleType"],"properties":{"scheduleType":{"$ref":"#/components/schemas/ScheduleTypeWithBasicEnum"},"cronExpression":{"type":"string"},"basicTiming":{"type":"string"}}},"ScheduleTypeWithBasicEnum":{"type":"string","enum":["manual","cron","basic"]},"GeographyEnum":{"type":"string","enum":["auto","us","eu"],"default":"auto"},"GeographyEnumNoDefault":{"type":"string","enum":["auto","us","eu"]},"ConnectionStatusEnum":{"type":"string","enum":["active","inactive","deprecated"]},"NamespaceDefinitionEnum":{"type":"string","description":"Define the location where the data will be stored in the destination","enum":["source","destination","custom_format"],"default":"destination"},"NonBreakingSchemaUpdatesBehaviorEnum":{"type":"string","description":"Set how Airbyte handles syncs when it detects a non-breaking schema change in the source","enum":["ignore","disable_connection","propagate_columns","propagate_fully"],"default":"ignore"},"NamespaceDefinitionEnumNoDefault":{"type":"string","description":"Define the location where the data will be stored in the destination","enum":["source","destination","custom_format"]},"NonBreakingSchemaUpdatesBehaviorEnumNoDefault":{"type":"string","description":"Set how Airbyte handles syncs when it detects a non-breaking schema change in the source","enum":["ignore","disable_connection","propagate_columns","propagate_fully"]},"DestinationResponse":{"title":"Root Type for DestinationResponse","description":"Provides details of a single destination.","type":"object","required":["destinationId","name","destinationType","workspaceId","configuration"],"properties":{"destinationId":{"format":"UUID","type":"string"},"name":{"type":"string"},"destinationType":{"type":"string"},"workspaceId":{"format":"UUID","type":"string"},"configuration":{"$ref":"#/components/schemas/DestinationConfiguration"}},"example":{"destinationId":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826","name":"Analytics Team Postgres","destinationType":"postgres","workspaceId":"871d9b60-11d1-44cb-8c92-c246d53bf87e"}},"SourceResponse":{"title":"Root Type for SourceResponse","description":"Provides details of a single source.","type":"object","required":["sourceId","name","sourceType","workspaceId","configuration"],"properties":{"sourceId":{"format":"UUID","type":"string"},"name":{"type":"string"},"sourceType":{"type":"string"},"workspaceId":{"format":"UUID","type":"string"},"configuration":{"$ref":"#/components/schemas/SourceConfiguration"}},"example":{"sourceId":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826","name":"Analytics Team Postgres","sourceType":"postgres","workspaceId":"871d9b60-11d1-44cb-8c92-c246d53bf87e"}},"DestinationCreateRequest":{"required":["name","workspaceId","configuration"],"type":"object","properties":{"name":{"type":"string"},"workspaceId":{"format":"uuid","type":"string"},"configuration":{"$ref":"#/components/schemas/DestinationConfiguration"}},"x-implements":"io.airbyte.public_api.server.helpers.ConfigurableActor","x-speakeasy-entity":"Destination","x-speakeasy-param-suppress-computed-diff":true},"DestinationPatchRequest":{"type":"object","properties":{"name":{"type":"string"},"configuration":{"$ref":"#/components/schemas/DestinationConfiguration"}},"x-implements":"io.airbyte.public_api.server.helpers.ConfigurableActor","x-speakeasy-entity":"Destination","x-speakeasy-param-suppress-computed-diff":true},"DestinationPutRequest":{"required":["name","configuration"],"type":"object","properties":{"name":{"type":"string"},"configuration":{"$ref":"#/components/schemas/DestinationConfiguration"}},"x-implements":"io.airbyte.public_api.server.helpers.ConfigurableActor","x-speakeasy-entity":"Destination","x-speakeasy-param-suppress-computed-diff":true},"WorkspaceCreateRequest":{"required":["name"],"type":"object","properties":{"name":{"description":"Name of the workspace","type":"string"}},"x-speakeasy-entity":"Workspace","x-speakeasy-param-suppress-computed-diff":true},"WorkspaceUpdateRequest":{"required":["name"],"type":"object","properties":{"name":{"description":"Name of the workspace","type":"string"}},"x-speakeasy-entity":"Workspace","x-speakeasy-param-suppress-computed-diff":true},"WorkspaceResponse":{"title":"Root Type for WorkspaceResponse","description":"Provides details of a single workspace.","type":"object","required":["workspaceId","name","dataResidency"],"properties":{"workspaceId":{"format":"UUID","type":"string"},"name":{"type":"string"},"dataResidency":{"$ref":"#/components/schemas/GeographyEnum"}},"x-speakeasy-entity":"Workspace","x-speakeasy-param-suppress-computed-diff":true},"ConnectionsResponse":{"title":"Root Type for ConnectionsResponse","description":"","required":["data"],"type":"object","properties":{"previous":{"type":"string"},"next":{"type":"string"},"data":{"type":"array","items":{"$ref":"#/components/schemas/ConnectionResponse"},"default":[]}},"example":{"next":"https://api.airbyte.com/v1/connections?limit=5&offset=10","previous":"https://api.airbyte.com/v1/connections?limit=5&offset=0","data":[{"name":"test-connection"},{"connection_id":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826"},{"sourceId":"49237019-645d-47d4-b45b-5eddf97775ce"},{"destinationId":"al312fs-0ab1-4f72-9ed7-0b8fc27c5826"},{"schedule":{"scheduleType":"manual"}},{"status":"active"},{"dataResidency":"auto"}]}},"SourcesResponse":{"title":"Root Type for SourcesResponse","description":"","required":["data"],"type":"object","properties":{"previous":{"type":"string"},"next":{"type":"string"},"data":{"type":"array","items":{"$ref":"#/components/schemas/SourceResponse"}}},"example":{"next":"https://api.airbyte.com/v1/sources?limit=5&offset=10","previous":"https://api.airbyte.com/v1/sources?limit=5&offset=0","data":{"sourceId":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826","name":"Analytics Team Postgres","sourceType":"postgres","workspaceId":"871d9b60-11d1-44cb-8c92-c246d53bf87e"}}},"DestinationsResponse":{"title":"Root Type for DestinationsResponse","description":"","required":["data"],"type":"object","properties":{"previous":{"type":"string"},"next":{"type":"string"},"data":{"type":"array","items":{"$ref":"#/components/schemas/DestinationResponse"}}},"example":{"next":"https://api.airbyte.com/v1/destinations?limit=5&offset=10","previous":"https://api.airbyte.com/v1/destinations?limit=5&offset=0","data":{"destinationId":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826","name":"Analytics Team Postgres","destinationType":"postgres","workspaceId":"871d9b60-11d1-44cb-8c92-c246d53bf87e"}}},"WorkspacesResponse":{"title":"Root Type for WorkspacesResponse","description":"","required":["data"],"type":"object","properties":{"previous":{"type":"string"},"next":{"type":"string"},"data":{"type":"array","items":{"$ref":"#/components/schemas/WorkspaceResponse"}}},"example":{"next":"https://api.airbyte.com/v1/workspaces?limit=5&offset=10","previous":"https://api.airbyte.com/v1/workspaces?limit=5&offset=0","data":{"workspaceId":"18dccc91-0ab1-4f72-9ed7-0b8fc27c5826","name":"Acme Company","dataResidency":"auto"}}},"StreamConfiguration":{"description":"Configurations for a single stream.","type":"object","required":["name"],"properties":{"name":{"type":"string"},"syncMode":{"$ref":"#/components/schemas/ConnectionSyncModeEnum"},"cursorField":{"description":"Path to the field that will be used to determine if a record is new or modified since the last sync. This field is REQUIRED if `sync_mode` is `incremental` unless there is a default.","type":"array","items":{"type":"string"}},"primaryKey":{"description":"Paths to the fields that will be used as primary key. This field is REQUIRED if `destination_sync_mode` is `*_dedup` unless it is already supplied by the source schema.","type":"array","items":{"type":"array","items":{"type":"string"}}}}},"StreamConfigurations":{"description":"A list of configured stream options for a connection.","type":"object","properties":{"streams":{"type":"array","items":{"$ref":"#/components/schemas/StreamConfiguration"}}}},"StreamPropertiesResponse":{"description":"A list of stream properties.","type":"object","properties":{"streams":{"type":"array","items":{"$ref":"#/components/schemas/StreamProperties"}}}},"StreamProperties":{"description":"The stream properties associated with a connection.","type":"object","properties":{"streamName":{"type":"string"},"syncModes":{"type":"array","items":{"$ref":"#/components/schemas/ConnectionSyncModeEnum"}},"defaultCursorField":{"type":"array","items":{"type":"string"}},"sourceDefinedCursorField":{"type":"boolean"},"sourceDefinedPrimaryKey":{"type":"array","items":{"type":"array","items":{"type":"string"}}},"propertyFields":{"type":"array","items":{"type":"array","items":{"type":"string"}}}}},"ConnectionSyncModeEnum":{"enum":["full_refresh_overwrite","full_refresh_append","incremental_append","incremental_deduped_history"]},"ActorTypeEnum":{"description":"Whether you're setting this override for a source or destination","enum":["source","destination"]},"destination-gcs":{"title":"GCS Destination Spec","type":"object","required":["gcs_bucket_name","gcs_bucket_path","credential","format","destinationType"],"properties":{"gcs_bucket_name":{"title":"GCS Bucket Name","order":1,"type":"string","description":"You can find the bucket name in the App Engine Admin console Application Settings page, under the label Google Cloud Storage Bucket. Read more <a href=\"https://cloud.google.com/storage/docs/naming-buckets\">here</a>.","examples":["airbyte_sync"]},"gcs_bucket_path":{"title":"GCS Bucket Path","description":"GCS Bucket Path string Subdirectory under the above bucket to sync the data into.","order":2,"type":"string","examples":["data_sync/test"]},"gcs_bucket_region":{"title":"GCS Bucket Region","type":"string","order":3,"default":"us","description":"Select a Region of the GCS Bucket. Read more <a href=\"https://cloud.google.com/storage/docs/locations\">here</a>.","enum":["northamerica-northeast1","northamerica-northeast2","us-central1","us-east1","us-east4","us-west1","us-west2","us-west3","us-west4","southamerica-east1","southamerica-west1","europe-central2","europe-north1","europe-west1","europe-west2","europe-west3","europe-west4","europe-west6","asia-east1","asia-east2","asia-northeast1","asia-northeast2","asia-northeast3","asia-south1","asia-south2","asia-southeast1","asia-southeast2","australia-southeast1","australia-southeast2","asia","eu","us","asia1","eur4","nam4"]},"credential":{"title":"Authentication","description":"An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href=\"https://cloud.google.com/storage/docs/authentication/hmackeys\">here</a>.","type":"object","order":0,"oneOf":[{"title":"HMAC Key","required":["credential_type","hmac_key_access_id","hmac_key_secret"],"properties":{"credential_type":{"type":"string","enum":["HMAC_KEY"],"default":"HMAC_KEY"},"hmac_key_access_id":{"type":"string","description":"When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long. Read more <a href=\"https://cloud.google.com/storage/docs/authentication/hmackeys#overview\">here</a>.","title":"Access ID","airbyte_secret":true,"order":0,"examples":["1234567890abcdefghij1234"]},"hmac_key_secret":{"type":"string","description":"The corresponding secret for the access ID. It is a 40-character base-64 encoded string.  Read more <a href=\"https://cloud.google.com/storage/docs/authentication/hmackeys#secrets\">here</a>.","title":"Secret","airbyte_secret":true,"order":1,"examples":["1234567890abcdefghij1234567890ABCDEFGHIJ"]}}}]},"format":{"title":"Output Format","type":"object","description":"Output data format. One of the following formats must be selected - <a href=\"https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro\">AVRO</a> format, <a href=\"https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet#parquet_schemas\">PARQUET</a> format, <a href=\"https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_table\">CSV</a> format, or <a href=\"https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json#loading_json_data_into_a_new_table\">JSONL</a> format.","order":4,"oneOf":[{"title":"Avro: Apache Avro","required":["format_type","compression_codec"],"properties":{"format_type":{"type":"string","enum":["Avro"],"default":"Avro"},"compression_codec":{"title":"Compression Codec","description":"The compression algorithm used to compress data. Default to no compression.","type":"object","oneOf":[{"title":"No Compression","required":["codec"],"properties":{"codec":{"type":"string","enum":["no compression"],"default":"no compression"}}},{"title":"Deflate","required":["codec"],"properties":{"codec":{"type":"string","enum":["Deflate"],"default":"Deflate"},"compression_level":{"title":"Deflate level","description":"0: no compression & fastest, 9: best compression & slowest.","type":"integer","default":0,"minimum":0,"maximum":9}}},{"title":"bzip2","required":["codec"],"properties":{"codec":{"type":"string","enum":["bzip2"],"default":"bzip2"}}},{"title":"xz","required":["codec"],"properties":{"codec":{"type":"string","enum":["xz"],"default":"xz"},"compression_level":{"title":"Compression Level","description":"The presets 0-3 are fast presets with medium compression. The presets 4-6 are fairly slow presets with high compression. The default preset is 6. The presets 7-9 are like the preset 6 but use bigger dictionaries and have higher compressor and decompressor memory requirements. Unless the uncompressed size of the file exceeds 8 MiB, 16 MiB, or 32 MiB, it is waste of memory to use the presets 7, 8, or 9, respectively. Read more <a href=\"https://commons.apache.org/proper/commons-compress/apidocs/org/apache/commons/compress/compressors/xz/XZCompressorOutputStream.html#XZCompressorOutputStream-java.io.OutputStream-int-\">here</a> for details.","type":"integer","default":6,"minimum":0,"maximum":9}}},{"title":"zstandard","required":["codec"],"properties":{"codec":{"type":"string","enum":["zstandard"],"default":"zstandard"},"compression_level":{"title":"Compression Level","description":"Negative levels are 'fast' modes akin to lz4 or snappy, levels above 9 are generally for archival purposes, and levels above 18 use a lot of memory.","type":"integer","default":3,"minimum":-5,"maximum":22},"include_checksum":{"title":"Include Checksum","description":"If true, include a checksum with each data block.","type":"boolean","default":false}}},{"title":"snappy","required":["codec"],"properties":{"codec":{"type":"string","enum":["snappy"],"default":"snappy"}}}]}}},{"title":"CSV: Comma-Separated Values","required":["format_type"],"properties":{"format_type":{"type":"string","enum":["CSV"],"default":"CSV"},"flattening":{"type":"string","title":"Normalization","description":"Whether the input JSON data should be normalized (flattened) in the output CSV. Please refer to docs for details.","default":"No flattening","enum":["No flattening","Root level flattening"]},"compression":{"title":"Compression","type":"object","description":"Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \".csv.gz\").","oneOf":[{"title":"No Compression","requires":["compression_type"],"properties":{"compression_type":{"type":"string","enum":["No Compression"],"default":"No Compression"}}},{"title":"GZIP","requires":["compression_type"],"properties":{"compression_type":{"type":"string","enum":["GZIP"],"default":"GZIP"}}}]}}},{"title":"JSON Lines: newline-delimited JSON","required":["format_type"],"properties":{"format_type":{"type":"string","enum":["JSONL"],"default":"JSONL"},"compression":{"title":"Compression","type":"object","description":"Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \".jsonl.gz\").","oneOf":[{"title":"No Compression","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["No Compression"],"default":"No Compression"}}},{"title":"GZIP","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["GZIP"],"default":"GZIP"}}}]}}},{"title":"Parquet: Columnar Storage","required":["format_type"],"properties":{"format_type":{"type":"string","enum":["Parquet"],"default":"Parquet"},"compression_codec":{"title":"Compression Codec","description":"The compression algorithm used to compress data pages.","type":"string","default":"UNCOMPRESSED","enum":["UNCOMPRESSED","SNAPPY","GZIP","LZO","BROTLI","LZ4","ZSTD"]},"block_size_mb":{"title":"Block Size (Row Group Size) (MB)","description":"This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. Default: 128 MB.","type":"integer","default":128,"examples":[128]},"max_padding_size_mb":{"title":"Max Padding Size (MB)","description":"Maximum size allowed as padding to align row groups. This is also the minimum size of a row group. Default: 8 MB.","type":"integer","default":8,"examples":[8]},"page_size_kb":{"title":"Page Size (KB)","description":"The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. Default: 1024 KB.","type":"integer","default":1024,"examples":[1024]},"dictionary_page_size_kb":{"title":"Dictionary Page Size (KB)","description":"There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. Default: 1024 KB.","type":"integer","default":1024,"examples":[1024]},"dictionary_encoding":{"title":"Dictionary Encoding","description":"Default: true.","type":"boolean","default":true}}}]},"destinationType":{"title":"gcs","const":"gcs","enum":["gcs"],"order":0,"type":"string"}}},"destination-gcs-update":{"title":"GCS Destination Spec","type":"object","required":["gcs_bucket_name","gcs_bucket_path","credential","format"],"properties":{"gcs_bucket_name":{"title":"GCS Bucket Name","order":1,"type":"string","description":"You can find the bucket name in the App Engine Admin console Application Settings page, under the label Google Cloud Storage Bucket. Read more <a href=\"https://cloud.google.com/storage/docs/naming-buckets\">here</a>.","examples":["airbyte_sync"]},"gcs_bucket_path":{"title":"GCS Bucket Path","description":"GCS Bucket Path string Subdirectory under the above bucket to sync the data into.","order":2,"type":"string","examples":["data_sync/test"]},"gcs_bucket_region":{"title":"GCS Bucket Region","type":"string","order":3,"default":"us","description":"Select a Region of the GCS Bucket. Read more <a href=\"https://cloud.google.com/storage/docs/locations\">here</a>.","enum":["northamerica-northeast1","northamerica-northeast2","us-central1","us-east1","us-east4","us-west1","us-west2","us-west3","us-west4","southamerica-east1","southamerica-west1","europe-central2","europe-north1","europe-west1","europe-west2","europe-west3","europe-west4","europe-west6","asia-east1","asia-east2","asia-northeast1","asia-northeast2","asia-northeast3","asia-south1","asia-south2","asia-southeast1","asia-southeast2","australia-southeast1","australia-southeast2","asia","eu","us","asia1","eur4","nam4"]},"credential":{"title":"Authentication","description":"An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href=\"https://cloud.google.com/storage/docs/authentication/hmackeys\">here</a>.","type":"object","order":0,"oneOf":[{"title":"HMAC Key","required":["credential_type","hmac_key_access_id","hmac_key_secret"],"properties":{"credential_type":{"type":"string","enum":["HMAC_KEY"],"default":"HMAC_KEY"},"hmac_key_access_id":{"type":"string","description":"When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long. Read more <a href=\"https://cloud.google.com/storage/docs/authentication/hmackeys#overview\">here</a>.","title":"Access ID","airbyte_secret":true,"order":0,"examples":["1234567890abcdefghij1234"]},"hmac_key_secret":{"type":"string","description":"The corresponding secret for the access ID. It is a 40-character base-64 encoded string.  Read more <a href=\"https://cloud.google.com/storage/docs/authentication/hmackeys#secrets\">here</a>.","title":"Secret","airbyte_secret":true,"order":1,"examples":["1234567890abcdefghij1234567890ABCDEFGHIJ"]}}}]},"format":{"title":"Output Format","type":"object","description":"Output data format. One of the following formats must be selected - <a href=\"https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro\">AVRO</a> format, <a href=\"https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet#parquet_schemas\">PARQUET</a> format, <a href=\"https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#loading_csv_data_into_a_table\">CSV</a> format, or <a href=\"https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json#loading_json_data_into_a_new_table\">JSONL</a> format.","order":4,"oneOf":[{"title":"Avro: Apache Avro","required":["format_type","compression_codec"],"properties":{"format_type":{"type":"string","enum":["Avro"],"default":"Avro"},"compression_codec":{"title":"Compression Codec","description":"The compression algorithm used to compress data. Default to no compression.","type":"object","oneOf":[{"title":"No Compression","required":["codec"],"properties":{"codec":{"type":"string","enum":["no compression"],"default":"no compression"}}},{"title":"Deflate","required":["codec"],"properties":{"codec":{"type":"string","enum":["Deflate"],"default":"Deflate"},"compression_level":{"title":"Deflate level","description":"0: no compression & fastest, 9: best compression & slowest.","type":"integer","default":0,"minimum":0,"maximum":9}}},{"title":"bzip2","required":["codec"],"properties":{"codec":{"type":"string","enum":["bzip2"],"default":"bzip2"}}},{"title":"xz","required":["codec"],"properties":{"codec":{"type":"string","enum":["xz"],"default":"xz"},"compression_level":{"title":"Compression Level","description":"The presets 0-3 are fast presets with medium compression. The presets 4-6 are fairly slow presets with high compression. The default preset is 6. The presets 7-9 are like the preset 6 but use bigger dictionaries and have higher compressor and decompressor memory requirements. Unless the uncompressed size of the file exceeds 8 MiB, 16 MiB, or 32 MiB, it is waste of memory to use the presets 7, 8, or 9, respectively. Read more <a href=\"https://commons.apache.org/proper/commons-compress/apidocs/org/apache/commons/compress/compressors/xz/XZCompressorOutputStream.html#XZCompressorOutputStream-java.io.OutputStream-int-\">here</a> for details.","type":"integer","default":6,"minimum":0,"maximum":9}}},{"title":"zstandard","required":["codec"],"properties":{"codec":{"type":"string","enum":["zstandard"],"default":"zstandard"},"compression_level":{"title":"Compression Level","description":"Negative levels are 'fast' modes akin to lz4 or snappy, levels above 9 are generally for archival purposes, and levels above 18 use a lot of memory.","type":"integer","default":3,"minimum":-5,"maximum":22},"include_checksum":{"title":"Include Checksum","description":"If true, include a checksum with each data block.","type":"boolean","default":false}}},{"title":"snappy","required":["codec"],"properties":{"codec":{"type":"string","enum":["snappy"],"default":"snappy"}}}]}}},{"title":"CSV: Comma-Separated Values","required":["format_type"],"properties":{"format_type":{"type":"string","enum":["CSV"],"default":"CSV"},"flattening":{"type":"string","title":"Normalization","description":"Whether the input JSON data should be normalized (flattened) in the output CSV. Please refer to docs for details.","default":"No flattening","enum":["No flattening","Root level flattening"]},"compression":{"title":"Compression","type":"object","description":"Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \".csv.gz\").","oneOf":[{"title":"No Compression","requires":["compression_type"],"properties":{"compression_type":{"type":"string","enum":["No Compression"],"default":"No Compression"}}},{"title":"GZIP","requires":["compression_type"],"properties":{"compression_type":{"type":"string","enum":["GZIP"],"default":"GZIP"}}}]}}},{"title":"JSON Lines: newline-delimited JSON","required":["format_type"],"properties":{"format_type":{"type":"string","enum":["JSONL"],"default":"JSONL"},"compression":{"title":"Compression","type":"object","description":"Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \".jsonl.gz\").","oneOf":[{"title":"No Compression","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["No Compression"],"default":"No Compression"}}},{"title":"GZIP","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["GZIP"],"default":"GZIP"}}}]}}},{"title":"Parquet: Columnar Storage","required":["format_type"],"properties":{"format_type":{"type":"string","enum":["Parquet"],"default":"Parquet"},"compression_codec":{"title":"Compression Codec","description":"The compression algorithm used to compress data pages.","type":"string","default":"UNCOMPRESSED","enum":["UNCOMPRESSED","SNAPPY","GZIP","LZO","BROTLI","LZ4","ZSTD"]},"block_size_mb":{"title":"Block Size (Row Group Size) (MB)","description":"This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. Default: 128 MB.","type":"integer","default":128,"examples":[128]},"max_padding_size_mb":{"title":"Max Padding Size (MB)","description":"Maximum size allowed as padding to align row groups. This is also the minimum size of a row group. Default: 8 MB.","type":"integer","default":8,"examples":[8]},"page_size_kb":{"title":"Page Size (KB)","description":"The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. Default: 1024 KB.","type":"integer","default":1024,"examples":[1024]},"dictionary_page_size_kb":{"title":"Dictionary Page Size (KB)","description":"There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. Default: 1024 KB.","type":"integer","default":1024,"examples":[1024]},"dictionary_encoding":{"title":"Dictionary Encoding","description":"Default: true.","type":"boolean","default":true}}}]}}},"destination-xata":{"title":"Destination Xata","type":"object","required":["api_key","db_url","destinationType"],"properties":{"api_key":{"title":"API Key","description":"API Key to connect.","type":"string","order":0,"airbyte_secret":true},"db_url":{"title":"Database URL","description":"URL pointing to your workspace.","type":"string","order":1,"example":"https://my-workspace-abc123.us-east-1.xata.sh/db/nyc-taxi-fares:main"},"destinationType":{"title":"xata","const":"xata","enum":["xata"],"order":0,"type":"string"}}},"destination-xata-update":{"title":"Destination Xata","type":"object","required":["api_key","db_url"],"properties":{"api_key":{"title":"API Key","description":"API Key to connect.","type":"string","order":0,"airbyte_secret":true},"db_url":{"title":"Database URL","description":"URL pointing to your workspace.","type":"string","order":1,"example":"https://my-workspace-abc123.us-east-1.xata.sh/db/nyc-taxi-fares:main"}}},"destination-clickhouse":{"title":"ClickHouse Destination Spec","type":"object","required":["host","port","database","username","destinationType"],"properties":{"host":{"title":"Host","description":"Hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"HTTP port of the database.","type":"integer","minimum":0,"maximum":65536,"default":8123,"examples":["8123"],"order":1},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":2},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":3},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":4},"jdbc_url_params":{"description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","title":"JDBC URL Params","type":"string","order":5},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]},"destinationType":{"title":"clickhouse","const":"clickhouse","enum":["clickhouse"],"order":0,"type":"string"}}},"destination-clickhouse-update":{"title":"ClickHouse Destination Spec","type":"object","required":["host","port","database","username"],"properties":{"host":{"title":"Host","description":"Hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"HTTP port of the database.","type":"integer","minimum":0,"maximum":65536,"default":8123,"examples":["8123"],"order":1},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":2},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":3},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":4},"jdbc_url_params":{"description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","title":"JDBC URL Params","type":"string","order":5},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]}}},"destination-mssql":{"title":"MS SQL Server Destination Spec","type":"object","required":["host","port","username","database","schema","destinationType"],"properties":{"host":{"title":"Host","description":"The host name of the MSSQL database.","type":"string","order":0},"port":{"title":"Port","description":"The port of the MSSQL database.","type":"integer","minimum":0,"maximum":65536,"default":1433,"examples":["1433"],"order":1},"database":{"title":"DB Name","description":"The name of the MSSQL database.","type":"string","order":2},"schema":{"title":"Default Schema","description":"The default schema tables are written to if the source does not specify a namespace. The usual value for this field is \"public\".","type":"string","examples":["public"],"default":"public","order":3},"username":{"title":"User","description":"The username which is used to access the database.","type":"string","order":4},"password":{"title":"Password","description":"The password associated with this username.","type":"string","airbyte_secret":true,"order":5},"jdbc_url_params":{"title":"JDBC URL Params","description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","type":"string","order":6},"ssl_method":{"title":"SSL Method","type":"object","description":"The encryption method which is used to communicate with the database.","order":7,"oneOf":[{"title":"Encrypted (trust server certificate)","description":"Use the certificate provided by the server without verification. (For testing purposes only!)","required":["ssl_method"],"type":"object","properties":{"ssl_method":{"type":"string","const":"encrypted_trust_server_certificate","enum":["encrypted_trust_server_certificate"],"default":"encrypted_trust_server_certificate"}}},{"title":"Encrypted (verify certificate)","description":"Verify and use the certificate provided by the server.","required":["ssl_method","trustStoreName","trustStorePassword"],"type":"object","properties":{"ssl_method":{"type":"string","const":"encrypted_verify_certificate","enum":["encrypted_verify_certificate"],"default":"encrypted_verify_certificate"},"hostNameInCertificate":{"title":"Host Name In Certificate","type":"string","description":"Specifies the host name of the server. The value of this property must match the subject property of the certificate.","order":8}}}]},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]},"destinationType":{"title":"mssql","const":"mssql","enum":["mssql"],"order":0,"type":"string"}}},"destination-mssql-update":{"title":"MS SQL Server Destination Spec","type":"object","required":["host","port","username","database","schema"],"properties":{"host":{"title":"Host","description":"The host name of the MSSQL database.","type":"string","order":0},"port":{"title":"Port","description":"The port of the MSSQL database.","type":"integer","minimum":0,"maximum":65536,"default":1433,"examples":["1433"],"order":1},"database":{"title":"DB Name","description":"The name of the MSSQL database.","type":"string","order":2},"schema":{"title":"Default Schema","description":"The default schema tables are written to if the source does not specify a namespace. The usual value for this field is \"public\".","type":"string","examples":["public"],"default":"public","order":3},"username":{"title":"User","description":"The username which is used to access the database.","type":"string","order":4},"password":{"title":"Password","description":"The password associated with this username.","type":"string","airbyte_secret":true,"order":5},"jdbc_url_params":{"title":"JDBC URL Params","description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","type":"string","order":6},"ssl_method":{"title":"SSL Method","type":"object","description":"The encryption method which is used to communicate with the database.","order":7,"oneOf":[{"title":"Encrypted (trust server certificate)","description":"Use the certificate provided by the server without verification. (For testing purposes only!)","required":["ssl_method"],"type":"object","properties":{"ssl_method":{"type":"string","const":"encrypted_trust_server_certificate","enum":["encrypted_trust_server_certificate"],"default":"encrypted_trust_server_certificate"}}},{"title":"Encrypted (verify certificate)","description":"Verify and use the certificate provided by the server.","required":["ssl_method","trustStoreName","trustStorePassword"],"type":"object","properties":{"ssl_method":{"type":"string","const":"encrypted_verify_certificate","enum":["encrypted_verify_certificate"],"default":"encrypted_verify_certificate"},"hostNameInCertificate":{"title":"Host Name In Certificate","type":"string","description":"Specifies the host name of the server. The value of this property must match the subject property of the certificate.","order":8}}}]},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]}}},"destination-mysql":{"title":"MySQL Destination Spec","type":"object","required":["host","port","username","database","destinationType"],"properties":{"host":{"title":"Host","description":"Hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"Port of the database.","type":"integer","minimum":0,"maximum":65536,"default":3306,"examples":["3306"],"order":1},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":2},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":3},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":4},"jdbc_url_params":{"description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","title":"JDBC URL Params","type":"string","order":6},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]},"destinationType":{"title":"mysql","const":"mysql","enum":["mysql"],"order":0,"type":"string"}}},"destination-mysql-update":{"title":"MySQL Destination Spec","type":"object","required":["host","port","username","database"],"properties":{"host":{"title":"Host","description":"Hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"Port of the database.","type":"integer","minimum":0,"maximum":65536,"default":3306,"examples":["3306"],"order":1},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":2},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":3},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":4},"jdbc_url_params":{"description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","title":"JDBC URL Params","type":"string","order":6},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]}}},"destination-pubsub":{"title":"Google PubSub Destination Spec","type":"object","required":["project_id","topic_id","credentials_json","ordering_enabled","batching_enabled","destinationType"],"properties":{"project_id":{"type":"string","description":"The GCP project ID for the project containing the target PubSub.","title":"Project ID"},"topic_id":{"type":"string","description":"The PubSub topic ID in the given GCP project ID.","title":"PubSub Topic ID"},"credentials_json":{"type":"string","description":"The contents of the JSON service account key. Check out the <a href=\"https://docs.airbyte.com/integrations/destinations/pubsub\">docs</a> if you need help generating this key.","title":"Credentials JSON","airbyte_secret":true},"ordering_enabled":{"title":"Message Ordering Enabled","description":"If TRUE PubSub publisher will have <a href=\"https://cloud.google.com/pubsub/docs/ordering\">message ordering</a> enabled. Every message will have an ordering key of stream","type":"boolean","default":false},"batching_enabled":{"type":"boolean","title":"Message Batching Enabled","description":"If TRUE messages will be buffered instead of sending them one by one","default":false},"batching_delay_threshold":{"type":"integer","title":"Message Batching: Delay Threshold","description":"Number of ms before the buffer is flushed","default":1,"minimum":1},"batching_element_count_threshold":{"type":"integer","title":"Message Batching: Element Count Threshold","description":"Number of messages before the buffer is flushed","default":1,"minimum":1},"batching_request_bytes_threshold":{"type":"integer","title":"Message Batching: Request Bytes Threshold","description":"Number of bytes before the buffer is flushed","default":1,"minimum":1},"destinationType":{"title":"pubsub","const":"pubsub","enum":["pubsub"],"order":0,"type":"string"}}},"destination-pubsub-update":{"title":"Google PubSub Destination Spec","type":"object","required":["project_id","topic_id","credentials_json","ordering_enabled","batching_enabled"],"properties":{"project_id":{"type":"string","description":"The GCP project ID for the project containing the target PubSub.","title":"Project ID"},"topic_id":{"type":"string","description":"The PubSub topic ID in the given GCP project ID.","title":"PubSub Topic ID"},"credentials_json":{"type":"string","description":"The contents of the JSON service account key. Check out the <a href=\"https://docs.airbyte.com/integrations/destinations/pubsub\">docs</a> if you need help generating this key.","title":"Credentials JSON","airbyte_secret":true},"ordering_enabled":{"title":"Message Ordering Enabled","description":"If TRUE PubSub publisher will have <a href=\"https://cloud.google.com/pubsub/docs/ordering\">message ordering</a> enabled. Every message will have an ordering key of stream","type":"boolean","default":false},"batching_enabled":{"type":"boolean","title":"Message Batching Enabled","description":"If TRUE messages will be buffered instead of sending them one by one","default":false},"batching_delay_threshold":{"type":"integer","title":"Message Batching: Delay Threshold","description":"Number of ms before the buffer is flushed","default":1,"minimum":1},"batching_element_count_threshold":{"type":"integer","title":"Message Batching: Element Count Threshold","description":"Number of messages before the buffer is flushed","default":1,"minimum":1},"batching_request_bytes_threshold":{"type":"integer","title":"Message Batching: Request Bytes Threshold","description":"Number of bytes before the buffer is flushed","default":1,"minimum":1}}},"destination-keen":{"title":"Keen Spec","type":"object","required":["project_id","api_key","destinationType"],"properties":{"project_id":{"description":"To get Keen Project ID, navigate to the Access tab from the left-hand, side panel and check the Project Details section.","title":"Project ID","type":"string","examples":["58b4acc22ba938934e888322e"]},"api_key":{"title":"API Key","description":"To get Keen Master API Key, navigate to the Access tab from the left-hand, side panel and check the Project Details section.","type":"string","examples":["ABCDEFGHIJKLMNOPRSTUWXYZ"],"airbyte_secret":true},"infer_timestamp":{"title":"Infer Timestamp","description":"Allow connector to guess keen.timestamp value based on the streamed data.","type":"boolean","default":true},"destinationType":{"title":"keen","const":"keen","enum":["keen"],"order":0,"type":"string"}}},"destination-keen-update":{"title":"Keen Spec","type":"object","required":["project_id","api_key"],"properties":{"project_id":{"description":"To get Keen Project ID, navigate to the Access tab from the left-hand, side panel and check the Project Details section.","title":"Project ID","type":"string","examples":["58b4acc22ba938934e888322e"]},"api_key":{"title":"API Key","description":"To get Keen Master API Key, navigate to the Access tab from the left-hand, side panel and check the Project Details section.","type":"string","examples":["ABCDEFGHIJKLMNOPRSTUWXYZ"],"airbyte_secret":true},"infer_timestamp":{"title":"Infer Timestamp","description":"Allow connector to guess keen.timestamp value based on the streamed data.","type":"boolean","default":true}}},"destination-mongodb":{"title":"MongoDB Destination Spec","type":"object","required":["database","auth_type","destinationType"],"properties":{"instance_type":{"description":"MongoDb instance to connect to. For MongoDB Atlas and Replica Set TLS connection is used by default.","title":"MongoDb Instance Type","type":"object","order":0,"oneOf":[{"title":"Standalone MongoDb Instance","required":["instance","host","port"],"properties":{"instance":{"type":"string","enum":["standalone"],"default":"standalone"},"host":{"title":"Host","type":"string","description":"The Host of a Mongo database to be replicated.","order":0},"port":{"title":"Port","type":"integer","description":"The Port of a Mongo database to be replicated.","minimum":0,"maximum":65536,"default":27017,"examples":["27017"],"order":1}}},{"title":"Replica Set","required":["instance","server_addresses"],"properties":{"instance":{"type":"string","enum":["replica"],"default":"replica"},"server_addresses":{"title":"Server addresses","type":"string","description":"The members of a replica set. Please specify `host`:`port` of each member seperated by comma.","examples":["host1:27017,host2:27017,host3:27017"],"order":0},"replica_set":{"title":"Replica Set","type":"string","description":"A replica set name.","order":1}}},{"title":"MongoDB Atlas","required":["instance","cluster_url"],"properties":{"instance":{"type":"string","enum":["atlas"],"default":"atlas"},"cluster_url":{"title":"Cluster URL","type":"string","description":"URL of a cluster to connect to.","order":0}}}]},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":2},"auth_type":{"title":"Authorization type","type":"object","description":"Authorization type.","oneOf":[{"title":"None","description":"None.","required":["authorization"],"type":"object","properties":{"authorization":{"type":"string","const":"none","enum":["none"]}}},{"title":"Login/Password","description":"Login/Password.","required":["authorization","username","password"],"type":"object","properties":{"authorization":{"type":"string","const":"login/password","enum":["login/password"]},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":1},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":2}}}]},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]},"destinationType":{"title":"mongodb","const":"mongodb","enum":["mongodb"],"order":0,"type":"string"}}},"destination-mongodb-update":{"title":"MongoDB Destination Spec","type":"object","required":["database","auth_type"],"properties":{"instance_type":{"description":"MongoDb instance to connect to. For MongoDB Atlas and Replica Set TLS connection is used by default.","title":"MongoDb Instance Type","type":"object","order":0,"oneOf":[{"title":"Standalone MongoDb Instance","required":["instance","host","port"],"properties":{"instance":{"type":"string","enum":["standalone"],"default":"standalone"},"host":{"title":"Host","type":"string","description":"The Host of a Mongo database to be replicated.","order":0},"port":{"title":"Port","type":"integer","description":"The Port of a Mongo database to be replicated.","minimum":0,"maximum":65536,"default":27017,"examples":["27017"],"order":1}}},{"title":"Replica Set","required":["instance","server_addresses"],"properties":{"instance":{"type":"string","enum":["replica"],"default":"replica"},"server_addresses":{"title":"Server addresses","type":"string","description":"The members of a replica set. Please specify `host`:`port` of each member seperated by comma.","examples":["host1:27017,host2:27017,host3:27017"],"order":0},"replica_set":{"title":"Replica Set","type":"string","description":"A replica set name.","order":1}}},{"title":"MongoDB Atlas","required":["instance","cluster_url"],"properties":{"instance":{"type":"string","enum":["atlas"],"default":"atlas"},"cluster_url":{"title":"Cluster URL","type":"string","description":"URL of a cluster to connect to.","order":0}}}]},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":2},"auth_type":{"title":"Authorization type","type":"object","description":"Authorization type.","oneOf":[{"title":"None","description":"None.","required":["authorization"],"type":"object","properties":{"authorization":{"type":"string","const":"none","enum":["none"]}}},{"title":"Login/Password","description":"Login/Password.","required":["authorization","username","password"],"type":"object","properties":{"authorization":{"type":"string","const":"login/password","enum":["login/password"]},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":1},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":2}}}]},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]}}},"destination-s3-glue":{"title":"S3 Destination Spec","type":"object","required":["s3_bucket_name","s3_bucket_path","s3_bucket_region","format","glue_database","glue_serialization_library","destinationType"],"properties":{"access_key_id":{"type":"string","description":"The access key ID to access the S3 bucket. Airbyte requires Read and Write permissions to the given bucket. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">here</a>.","title":"S3 Key ID","airbyte_secret":true,"examples":["A012345678910EXAMPLE"],"order":0},"secret_access_key":{"type":"string","description":"The corresponding secret to the access key ID. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">here</a>","title":"S3 Access Key","airbyte_secret":true,"examples":["a012345678910ABCDEFGH/AbCdEfGhEXAMPLEKEY"],"order":1},"s3_bucket_name":{"title":"S3 Bucket Name","type":"string","description":"The name of the S3 bucket. Read more <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html\">here</a>.","examples":["airbyte_sync"],"order":2},"s3_bucket_path":{"title":"S3 Bucket Path","description":"Directory under the S3 bucket where data will be written. Read more <a href=\"https://docs.airbyte.com/integrations/destinations/s3#:~:text=to%20format%20the-,bucket%20path,-%3A\">here</a>","type":"string","examples":["data_sync/test"],"order":3},"s3_bucket_region":{"title":"S3 Bucket Region","type":"string","default":"","description":"The region of the S3 bucket. See <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions\">here</a> for all region codes.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1","us-gov-east-1","us-gov-west-1"],"order":4},"format":{"title":"Output Format","type":"object","description":"Format of the data output. See <a href=\"https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema\">here</a> for more details","oneOf":[{"title":"JSON Lines: Newline-delimited JSON","required":["format_type"],"properties":{"format_type":{"title":"Format Type","type":"string","enum":["JSONL"],"default":"JSONL"},"compression":{"title":"Compression","type":"object","description":"Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \".jsonl.gz\").","oneOf":[{"title":"No Compression","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["No Compression"],"default":"No Compression"}}},{"title":"GZIP","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["GZIP"],"default":"GZIP"}}}]},"flattening":{"type":"string","title":"Flattening","description":"Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details.","default":"Root level flattening","enum":["No flattening","Root level flattening"]}}}],"order":5},"s3_endpoint":{"title":"Endpoint","type":"string","default":"","description":"Your S3 endpoint url. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html#:~:text=Service%20endpoints-,Amazon%20S3%20endpoints,-When%20you%20use\">here</a>","examples":["http://localhost:9000"],"order":6},"s3_path_format":{"title":"S3 Path Format","description":"Format string on how data will be organized inside the S3 bucket directory. Read more <a href=\"https://docs.airbyte.com/integrations/destinations/s3#:~:text=The%20full%20path%20of%20the%20output%20data%20with%20the%20default%20S3%20path%20format\">here</a>","type":"string","examples":["${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_"],"order":7},"file_name_pattern":{"type":"string","description":"The pattern allows you to set the file-name format for the S3 staging file(s)","title":"S3 Filename pattern","examples":["{date}","{date:yyyy_MM}","{timestamp}","{part_number}","{sync_id}"],"order":8},"glue_database":{"type":"string","description":"Name of the glue database for creating the tables, leave blank if no integration","title":"Glue database name","examples":["airbyte_database"],"order":9},"glue_serialization_library":{"title":"Serialization Library","description":"The library that your query engine will use for reading and writing data in your lake.","type":"string","enum":["org.openx.data.jsonserde.JsonSerDe","org.apache.hive.hcatalog.data.JsonSerDe"],"default":"org.openx.data.jsonserde.JsonSerDe","order":10},"destinationType":{"title":"s3-glue","const":"s3-glue","enum":["s3-glue"],"order":0,"type":"string"}}},"destination-s3-glue-update":{"title":"S3 Destination Spec","type":"object","required":["s3_bucket_name","s3_bucket_path","s3_bucket_region","format","glue_database","glue_serialization_library"],"properties":{"access_key_id":{"type":"string","description":"The access key ID to access the S3 bucket. Airbyte requires Read and Write permissions to the given bucket. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">here</a>.","title":"S3 Key ID","airbyte_secret":true,"examples":["A012345678910EXAMPLE"],"order":0},"secret_access_key":{"type":"string","description":"The corresponding secret to the access key ID. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">here</a>","title":"S3 Access Key","airbyte_secret":true,"examples":["a012345678910ABCDEFGH/AbCdEfGhEXAMPLEKEY"],"order":1},"s3_bucket_name":{"title":"S3 Bucket Name","type":"string","description":"The name of the S3 bucket. Read more <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html\">here</a>.","examples":["airbyte_sync"],"order":2},"s3_bucket_path":{"title":"S3 Bucket Path","description":"Directory under the S3 bucket where data will be written. Read more <a href=\"https://docs.airbyte.com/integrations/destinations/s3#:~:text=to%20format%20the-,bucket%20path,-%3A\">here</a>","type":"string","examples":["data_sync/test"],"order":3},"s3_bucket_region":{"title":"S3 Bucket Region","type":"string","default":"","description":"The region of the S3 bucket. See <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions\">here</a> for all region codes.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1","us-gov-east-1","us-gov-west-1"],"order":4},"format":{"title":"Output Format","type":"object","description":"Format of the data output. See <a href=\"https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema\">here</a> for more details","oneOf":[{"title":"JSON Lines: Newline-delimited JSON","required":["format_type"],"properties":{"format_type":{"title":"Format Type","type":"string","enum":["JSONL"],"default":"JSONL"},"compression":{"title":"Compression","type":"object","description":"Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \".jsonl.gz\").","oneOf":[{"title":"No Compression","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["No Compression"],"default":"No Compression"}}},{"title":"GZIP","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["GZIP"],"default":"GZIP"}}}]},"flattening":{"type":"string","title":"Flattening","description":"Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details.","default":"Root level flattening","enum":["No flattening","Root level flattening"]}}}],"order":5},"s3_endpoint":{"title":"Endpoint","type":"string","default":"","description":"Your S3 endpoint url. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html#:~:text=Service%20endpoints-,Amazon%20S3%20endpoints,-When%20you%20use\">here</a>","examples":["http://localhost:9000"],"order":6},"s3_path_format":{"title":"S3 Path Format","description":"Format string on how data will be organized inside the S3 bucket directory. Read more <a href=\"https://docs.airbyte.com/integrations/destinations/s3#:~:text=The%20full%20path%20of%20the%20output%20data%20with%20the%20default%20S3%20path%20format\">here</a>","type":"string","examples":["${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_"],"order":7},"file_name_pattern":{"type":"string","description":"The pattern allows you to set the file-name format for the S3 staging file(s)","title":"S3 Filename pattern","examples":["{date}","{date:yyyy_MM}","{timestamp}","{part_number}","{sync_id}"],"order":8},"glue_database":{"type":"string","description":"Name of the glue database for creating the tables, leave blank if no integration","title":"Glue database name","examples":["airbyte_database"],"order":9},"glue_serialization_library":{"title":"Serialization Library","description":"The library that your query engine will use for reading and writing data in your lake.","type":"string","enum":["org.openx.data.jsonserde.JsonSerDe","org.apache.hive.hcatalog.data.JsonSerDe"],"default":"org.openx.data.jsonserde.JsonSerDe","order":10}}},"destination-dev-null":{"title":"E2E Test (/dev/null) Destination Spec","type":"object","required":["test_destination","destinationType"],"properties":{"test_destination":{"title":"Test Destination","type":"object","description":"The type of destination to be used","oneOf":[{"title":"Silent","required":["test_destination_type"],"properties":{"test_destination_type":{"type":"string","const":"SILENT","default":"SILENT","enum":["SILENT"]}}}]},"destinationType":{"title":"dev-null","const":"dev-null","enum":["dev-null"],"order":0,"type":"string"}}},"destination-dev-null-update":{"title":"E2E Test (/dev/null) Destination Spec","type":"object","required":["test_destination"],"properties":{"test_destination":{"title":"Test Destination","type":"object","description":"The type of destination to be used","oneOf":[{"title":"Silent","required":["test_destination_type"],"properties":{"test_destination_type":{"type":"string","const":"SILENT","default":"SILENT","enum":["SILENT"]}}}]}}},"destination-timeplus":{"title":"Destination Timeplus","type":"object","required":["endpoint","apikey","destinationType"],"properties":{"endpoint":{"title":"Endpoint","description":"Timeplus workspace endpoint","type":"string","default":"https://us.timeplus.cloud/<workspace_id>","examples":["https://us.timeplus.cloud/workspace_id"],"order":0},"apikey":{"title":"API key","description":"Personal API key","type":"string","airbyte_secret":true,"order":1},"destinationType":{"title":"timeplus","const":"timeplus","enum":["timeplus"],"order":0,"type":"string"}}},"destination-timeplus-update":{"title":"Destination Timeplus","type":"object","required":["endpoint","apikey"],"properties":{"endpoint":{"title":"Endpoint","description":"Timeplus workspace endpoint","type":"string","default":"https://us.timeplus.cloud/<workspace_id>","examples":["https://us.timeplus.cloud/workspace_id"],"order":0},"apikey":{"title":"API key","description":"Personal API key","type":"string","airbyte_secret":true,"order":1}}},"destination-convex":{"title":"Destination Convex","type":"object","required":["deployment_url","access_key","destinationType"],"properties":{"deployment_url":{"type":"string","description":"URL of the Convex deployment that is the destination","examples":["https://murky-swan-635.convex.cloud","https://cluttered-owl-337.convex.cloud"]},"access_key":{"type":"string","description":"API access key used to send data to a Convex deployment.","airbyte_secret":"true"},"destinationType":{"title":"convex","const":"convex","enum":["convex"],"order":0,"type":"string"}}},"destination-convex-update":{"title":"Destination Convex","type":"object","required":["deployment_url","access_key"],"properties":{"deployment_url":{"type":"string","description":"URL of the Convex deployment that is the destination","examples":["https://murky-swan-635.convex.cloud","https://cluttered-owl-337.convex.cloud"]},"access_key":{"type":"string","description":"API access key used to send data to a Convex deployment.","airbyte_secret":"true"}}},"destination-firestore":{"title":"Destination Google Firestore","type":"object","required":["project_id","destinationType"],"properties":{"project_id":{"type":"string","description":"The GCP project ID for the project containing the target BigQuery dataset.","title":"Project ID"},"credentials_json":{"type":"string","description":"The contents of the JSON service account key. Check out the <a href=\"https://docs.airbyte.io/integrations/destinations/firestore\">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.","title":"Credentials JSON","airbyte_secret":true},"destinationType":{"title":"firestore","const":"firestore","enum":["firestore"],"order":0,"type":"string"}}},"destination-firestore-update":{"title":"Destination Google Firestore","type":"object","required":["project_id"],"properties":{"project_id":{"type":"string","description":"The GCP project ID for the project containing the target BigQuery dataset.","title":"Project ID"},"credentials_json":{"type":"string","description":"The contents of the JSON service account key. Check out the <a href=\"https://docs.airbyte.io/integrations/destinations/firestore\">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.","title":"Credentials JSON","airbyte_secret":true}}},"destination-redshift":{"title":"Redshift Destination Spec","type":"object","required":["host","port","database","username","password","schema","destinationType"],"properties":{"host":{"description":"Host Endpoint of the Redshift Cluster (must include the cluster-id, region and end with .redshift.amazonaws.com)","type":"string","title":"Host","order":1},"port":{"description":"Port of the database.","type":"integer","minimum":0,"maximum":65536,"default":5439,"examples":["5439"],"title":"Port","order":2},"username":{"description":"Username to use to access the database.","type":"string","title":"Username","order":3},"password":{"description":"Password associated with the username.","type":"string","airbyte_secret":true,"title":"Password","order":4},"database":{"description":"Name of the database.","type":"string","title":"Database","order":5},"schema":{"description":"The default schema tables are written to if the source does not specify a namespace. Unless specifically configured, the usual value for this field is \"public\".","type":"string","examples":["public"],"default":"public","title":"Default Schema","order":6},"jdbc_url_params":{"title":"JDBC URL Params","description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","type":"string","order":7},"uploading_method":{"title":"Uploading Method","type":"object","description":"The method how the data will be uploaded to the database.","order":8,"oneOf":[{"title":"Standard","required":["method"],"properties":{"method":{"type":"string","const":"Standard","enum":["Standard"]}}},{"title":"S3 Staging","required":["method","s3_bucket_name","s3_bucket_region","access_key_id","secret_access_key"],"properties":{"method":{"type":"string","const":"S3 Staging","enum":["S3 Staging"]},"s3_bucket_name":{"title":"S3 Bucket Name","type":"string","description":"The name of the staging S3 bucket to use if utilising a COPY strategy. COPY is recommended for production workloads for better speed and scalability. See <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html\">AWS docs</a> for more details.","examples":["airbyte.staging"]},"s3_bucket_path":{"title":"S3 Bucket Path","type":"string","description":"The directory under the S3 bucket where data will be written. If not provided, then defaults to the root directory. See <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/faq.html#:~:text=be%20globally%20unique.-,For%20S3%20bucket%20paths,-%2C%20you%20can%20use\">path's name recommendations</a> for more details.","examples":["data_sync/test"]},"s3_bucket_region":{"title":"S3 Bucket Region","type":"string","default":"","description":"The region of the S3 staging bucket to use if utilising a COPY strategy. See <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html#:~:text=In-,Region,-%2C%20choose%20the%20AWS\">AWS docs</a> for details.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1"]},"file_name_pattern":{"type":"string","description":"The pattern allows you to set the file-name format for the S3 staging file(s)","title":"S3 Filename pattern","examples":["{date}","{date:yyyy_MM}","{timestamp}","{part_number}","{sync_id}"],"order":8},"access_key_id":{"type":"string","description":"This ID grants access to the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket. See <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">AWS docs</a> on how to generate an access key ID and secret access key.","title":"S3 Key Id","airbyte_secret":true},"secret_access_key":{"type":"string","description":"The corresponding secret to the above access key id. See <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">AWS docs</a> on how to generate an access key ID and secret access key.","title":"S3 Access Key","airbyte_secret":true},"purge_staging_data":{"title":"Purge Staging Files and Tables","type":"boolean","description":"Whether to delete the staging files from S3 after completing the sync. See <a href=\"https://docs.airbyte.com/integrations/destinations/redshift/#:~:text=the%20root%20directory.-,Purge%20Staging%20Data,-Whether%20to%20delete\"> docs</a> for details.","default":true},"encryption":{"title":"Encryption","type":"object","description":"How to encrypt the staging data","default":{"encryption_type":"none"},"oneOf":[{"title":"No encryption","description":"Staging data will be stored in plaintext.","type":"object","required":["encryption_type"],"properties":{"encryption_type":{"type":"string","const":"none","enum":["none"],"default":"none"}}},{"title":"AES-CBC envelope encryption","description":"Staging data will be encrypted using AES-CBC envelope encryption.","type":"object","required":["encryption_type"],"properties":{"encryption_type":{"type":"string","const":"aes_cbc_envelope","enum":["aes_cbc_envelope"],"default":"aes_cbc_envelope"},"key_encrypting_key":{"type":"string","title":"Key","description":"The key, base64-encoded. Must be either 128, 192, or 256 bits. Leave blank to have Airbyte generate an ephemeral key for each sync.","airbyte_secret":true}}}]},"file_buffer_count":{"title":"File Buffer Count","type":"integer","minimum":10,"maximum":50,"default":10,"description":"Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects","examples":["10"]}}}]},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]},"destinationType":{"title":"redshift","const":"redshift","enum":["redshift"],"order":0,"type":"string"}}},"destination-redshift-update":{"title":"Redshift Destination Spec","type":"object","required":["host","port","database","username","password","schema"],"properties":{"host":{"description":"Host Endpoint of the Redshift Cluster (must include the cluster-id, region and end with .redshift.amazonaws.com)","type":"string","title":"Host","order":1},"port":{"description":"Port of the database.","type":"integer","minimum":0,"maximum":65536,"default":5439,"examples":["5439"],"title":"Port","order":2},"username":{"description":"Username to use to access the database.","type":"string","title":"Username","order":3},"password":{"description":"Password associated with the username.","type":"string","airbyte_secret":true,"title":"Password","order":4},"database":{"description":"Name of the database.","type":"string","title":"Database","order":5},"schema":{"description":"The default schema tables are written to if the source does not specify a namespace. Unless specifically configured, the usual value for this field is \"public\".","type":"string","examples":["public"],"default":"public","title":"Default Schema","order":6},"jdbc_url_params":{"title":"JDBC URL Params","description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","type":"string","order":7},"uploading_method":{"title":"Uploading Method","type":"object","description":"The method how the data will be uploaded to the database.","order":8,"oneOf":[{"title":"Standard","required":["method"],"properties":{"method":{"type":"string","const":"Standard","enum":["Standard"]}}},{"title":"S3 Staging","required":["method","s3_bucket_name","s3_bucket_region","access_key_id","secret_access_key"],"properties":{"method":{"type":"string","const":"S3 Staging","enum":["S3 Staging"]},"s3_bucket_name":{"title":"S3 Bucket Name","type":"string","description":"The name of the staging S3 bucket to use if utilising a COPY strategy. COPY is recommended for production workloads for better speed and scalability. See <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html\">AWS docs</a> for more details.","examples":["airbyte.staging"]},"s3_bucket_path":{"title":"S3 Bucket Path","type":"string","description":"The directory under the S3 bucket where data will be written. If not provided, then defaults to the root directory. See <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/defining-bucket-names-data-lakes/faq.html#:~:text=be%20globally%20unique.-,For%20S3%20bucket%20paths,-%2C%20you%20can%20use\">path's name recommendations</a> for more details.","examples":["data_sync/test"]},"s3_bucket_region":{"title":"S3 Bucket Region","type":"string","default":"","description":"The region of the S3 staging bucket to use if utilising a COPY strategy. See <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html#:~:text=In-,Region,-%2C%20choose%20the%20AWS\">AWS docs</a> for details.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1"]},"file_name_pattern":{"type":"string","description":"The pattern allows you to set the file-name format for the S3 staging file(s)","title":"S3 Filename pattern","examples":["{date}","{date:yyyy_MM}","{timestamp}","{part_number}","{sync_id}"],"order":8},"access_key_id":{"type":"string","description":"This ID grants access to the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket. See <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">AWS docs</a> on how to generate an access key ID and secret access key.","title":"S3 Key Id","airbyte_secret":true},"secret_access_key":{"type":"string","description":"The corresponding secret to the above access key id. See <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">AWS docs</a> on how to generate an access key ID and secret access key.","title":"S3 Access Key","airbyte_secret":true},"purge_staging_data":{"title":"Purge Staging Files and Tables","type":"boolean","description":"Whether to delete the staging files from S3 after completing the sync. See <a href=\"https://docs.airbyte.com/integrations/destinations/redshift/#:~:text=the%20root%20directory.-,Purge%20Staging%20Data,-Whether%20to%20delete\"> docs</a> for details.","default":true},"encryption":{"title":"Encryption","type":"object","description":"How to encrypt the staging data","default":{"encryption_type":"none"},"oneOf":[{"title":"No encryption","description":"Staging data will be stored in plaintext.","type":"object","required":["encryption_type"],"properties":{"encryption_type":{"type":"string","const":"none","enum":["none"],"default":"none"}}},{"title":"AES-CBC envelope encryption","description":"Staging data will be encrypted using AES-CBC envelope encryption.","type":"object","required":["encryption_type"],"properties":{"encryption_type":{"type":"string","const":"aes_cbc_envelope","enum":["aes_cbc_envelope"],"default":"aes_cbc_envelope"},"key_encrypting_key":{"type":"string","title":"Key","description":"The key, base64-encoded. Must be either 128, 192, or 256 bits. Leave blank to have Airbyte generate an ephemeral key for each sync.","airbyte_secret":true}}}]},"file_buffer_count":{"title":"File Buffer Count","type":"integer","minimum":10,"maximum":50,"default":10,"description":"Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects","examples":["10"]}}}]},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]}}},"destination-dynamodb":{"title":"DynamoDB Destination Spec","type":"object","required":["dynamodb_table_name_prefix","dynamodb_region","access_key_id","secret_access_key","destinationType"],"properties":{"dynamodb_endpoint":{"title":"Endpoint","type":"string","default":"","description":"This is your DynamoDB endpoint url.(if you are working with AWS DynamoDB, just leave empty).","examples":["http://localhost:9000"]},"dynamodb_table_name_prefix":{"title":"Table name prefix","type":"string","description":"The prefix to use when naming DynamoDB tables.","examples":["airbyte_sync"]},"dynamodb_region":{"title":"DynamoDB Region","type":"string","default":"","description":"The region of the DynamoDB.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1","us-gov-east-1","us-gov-west-1"]},"access_key_id":{"type":"string","description":"The access key id to access the DynamoDB. Airbyte requires Read and Write permissions to the DynamoDB.","title":"DynamoDB Key Id","airbyte_secret":true,"examples":["A012345678910EXAMPLE"]},"secret_access_key":{"type":"string","description":"The corresponding secret to the access key id.","title":"DynamoDB Access Key","airbyte_secret":true,"examples":["a012345678910ABCDEFGH/AbCdEfGhEXAMPLEKEY"]},"destinationType":{"title":"dynamodb","const":"dynamodb","enum":["dynamodb"],"order":0,"type":"string"}}},"destination-dynamodb-update":{"title":"DynamoDB Destination Spec","type":"object","required":["dynamodb_table_name_prefix","dynamodb_region","access_key_id","secret_access_key"],"properties":{"dynamodb_endpoint":{"title":"Endpoint","type":"string","default":"","description":"This is your DynamoDB endpoint url.(if you are working with AWS DynamoDB, just leave empty).","examples":["http://localhost:9000"]},"dynamodb_table_name_prefix":{"title":"Table name prefix","type":"string","description":"The prefix to use when naming DynamoDB tables.","examples":["airbyte_sync"]},"dynamodb_region":{"title":"DynamoDB Region","type":"string","default":"","description":"The region of the DynamoDB.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1","us-gov-east-1","us-gov-west-1"]},"access_key_id":{"type":"string","description":"The access key id to access the DynamoDB. Airbyte requires Read and Write permissions to the DynamoDB.","title":"DynamoDB Key Id","airbyte_secret":true,"examples":["A012345678910EXAMPLE"]},"secret_access_key":{"type":"string","description":"The corresponding secret to the access key id.","title":"DynamoDB Access Key","airbyte_secret":true,"examples":["a012345678910ABCDEFGH/AbCdEfGhEXAMPLEKEY"]}}},"destination-snowflake":{"title":"Snowflake Destination Spec","type":"object","required":["host","role","warehouse","database","schema","username","destinationType"],"properties":{"host":{"description":"Enter your Snowflake account's <a href=\"https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#using-an-account-locator-as-an-identifier\">locator</a> (in the format <account_locator>.<region>.<cloud>.snowflakecomputing.com)","examples":["accountname.us-east-2.aws.snowflakecomputing.com","accountname.snowflakecomputing.com"],"type":"string","title":"Host","pattern":"^(http(s)?:\\/\\/)?([^./?#]+\\.)?([^./?#]+\\.)?([^./?#]+\\.)?([^./?#]+\\.snowflakecomputing\\.com)$","pattern_descriptor":"{account_name}.snowflakecomputing.com or {accountname}.{aws_location}.aws.snowflakecomputing.com","order":0},"role":{"description":"Enter the <a href=\"https://docs.snowflake.com/en/user-guide/security-access-control-overview.html#roles\">role</a> that you want to use to access Snowflake","examples":["AIRBYTE_ROLE"],"type":"string","title":"Role","order":1},"warehouse":{"description":"Enter the name of the <a href=\"https://docs.snowflake.com/en/user-guide/warehouses-overview.html#overview-of-warehouses\">warehouse</a> that you want to sync data into","examples":["AIRBYTE_WAREHOUSE"],"type":"string","title":"Warehouse","order":2},"database":{"description":"Enter the name of the <a href=\"https://docs.snowflake.com/en/sql-reference/ddl-database.html#database-schema-share-ddl\">database</a> you want to sync data into","examples":["AIRBYTE_DATABASE"],"type":"string","title":"Database","order":3},"schema":{"description":"Enter the name of the default <a href=\"https://docs.snowflake.com/en/sql-reference/ddl-database.html#database-schema-share-ddl\">schema</a>","examples":["AIRBYTE_SCHEMA"],"type":"string","title":"Default Schema","order":4},"username":{"description":"Enter the name of the user you want to use to access the database","examples":["AIRBYTE_USER"],"type":"string","title":"Username","order":5},"credentials":{"title":"Authorization Method","description":"","type":"object","oneOf":[{"title":"OAuth2.0","type":"object","order":0,"required":["access_token","refresh_token"],"properties":{"auth_type":{"type":"string","const":"OAuth2.0","enum":["OAuth2.0"],"default":"OAuth2.0","order":0},"client_id":{"type":"string","title":"Client ID","description":"Enter your application's Client ID","airbyte_secret":true},"client_secret":{"type":"string","title":"Client Secret","description":"Enter your application's Client secret","airbyte_secret":true},"access_token":{"type":"string","title":"Access Token","description":"Enter you application's Access Token","airbyte_secret":true},"refresh_token":{"type":"string","title":"Refresh Token","description":"Enter your application's Refresh Token","airbyte_secret":true}}},{"title":"Key Pair Authentication","type":"object","order":1,"required":["private_key"],"properties":{"auth_type":{"type":"string","const":"Key Pair Authentication","enum":["Key Pair Authentication"],"default":"Key Pair Authentication","order":0},"private_key":{"type":"string","title":"Private Key","description":"RSA Private key to use for Snowflake connection. See the <a href=\"https://docs.airbyte.com/integrations/destinations/snowflake\">docs</a> for more information on how to obtain this key.","multiline":true,"airbyte_secret":true},"private_key_password":{"type":"string","title":"Passphrase","description":"Passphrase for private key","airbyte_secret":true}}},{"title":"Username and Password","type":"object","required":["password"],"order":2,"properties":{"auth_type":{"type":"string","const":"Username and Password","enum":["Username and Password"],"default":"Username and Password","order":0},"password":{"description":"Enter the password associated with the username.","type":"string","airbyte_secret":true,"title":"Password","order":1}}}],"order":6},"jdbc_url_params":{"description":"Enter the additional properties to pass to the JDBC URL string when connecting to the database (formatted as key=value pairs separated by the symbol &). Example: key1=value1&key2=value2&key3=value3","title":"JDBC URL Params","type":"string","order":7},"raw_data_schema":{"type":"string","description":"The schema to write raw tables into","title":"Destinations V2 Raw Table Schema","order":10},"destinationType":{"title":"snowflake","const":"snowflake","enum":["snowflake"],"order":0,"type":"string"}}},"destination-snowflake-update":{"title":"Snowflake Destination Spec","type":"object","required":["host","role","warehouse","database","schema","username"],"properties":{"host":{"description":"Enter your Snowflake account's <a href=\"https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#using-an-account-locator-as-an-identifier\">locator</a> (in the format <account_locator>.<region>.<cloud>.snowflakecomputing.com)","examples":["accountname.us-east-2.aws.snowflakecomputing.com","accountname.snowflakecomputing.com"],"type":"string","title":"Host","pattern":"^(http(s)?:\\/\\/)?([^./?#]+\\.)?([^./?#]+\\.)?([^./?#]+\\.)?([^./?#]+\\.snowflakecomputing\\.com)$","pattern_descriptor":"{account_name}.snowflakecomputing.com or {accountname}.{aws_location}.aws.snowflakecomputing.com","order":0},"role":{"description":"Enter the <a href=\"https://docs.snowflake.com/en/user-guide/security-access-control-overview.html#roles\">role</a> that you want to use to access Snowflake","examples":["AIRBYTE_ROLE"],"type":"string","title":"Role","order":1},"warehouse":{"description":"Enter the name of the <a href=\"https://docs.snowflake.com/en/user-guide/warehouses-overview.html#overview-of-warehouses\">warehouse</a> that you want to sync data into","examples":["AIRBYTE_WAREHOUSE"],"type":"string","title":"Warehouse","order":2},"database":{"description":"Enter the name of the <a href=\"https://docs.snowflake.com/en/sql-reference/ddl-database.html#database-schema-share-ddl\">database</a> you want to sync data into","examples":["AIRBYTE_DATABASE"],"type":"string","title":"Database","order":3},"schema":{"description":"Enter the name of the default <a href=\"https://docs.snowflake.com/en/sql-reference/ddl-database.html#database-schema-share-ddl\">schema</a>","examples":["AIRBYTE_SCHEMA"],"type":"string","title":"Default Schema","order":4},"username":{"description":"Enter the name of the user you want to use to access the database","examples":["AIRBYTE_USER"],"type":"string","title":"Username","order":5},"credentials":{"title":"Authorization Method","description":"","type":"object","oneOf":[{"title":"OAuth2.0","type":"object","order":0,"required":["access_token","refresh_token"],"properties":{"auth_type":{"type":"string","const":"OAuth2.0","enum":["OAuth2.0"],"default":"OAuth2.0","order":0},"client_id":{"type":"string","title":"Client ID","description":"Enter your application's Client ID","airbyte_secret":true},"client_secret":{"type":"string","title":"Client Secret","description":"Enter your application's Client secret","airbyte_secret":true},"access_token":{"type":"string","title":"Access Token","description":"Enter you application's Access Token","airbyte_secret":true},"refresh_token":{"type":"string","title":"Refresh Token","description":"Enter your application's Refresh Token","airbyte_secret":true}}},{"title":"Key Pair Authentication","type":"object","order":1,"required":["private_key"],"properties":{"auth_type":{"type":"string","const":"Key Pair Authentication","enum":["Key Pair Authentication"],"default":"Key Pair Authentication","order":0},"private_key":{"type":"string","title":"Private Key","description":"RSA Private key to use for Snowflake connection. See the <a href=\"https://docs.airbyte.com/integrations/destinations/snowflake\">docs</a> for more information on how to obtain this key.","multiline":true,"airbyte_secret":true},"private_key_password":{"type":"string","title":"Passphrase","description":"Passphrase for private key","airbyte_secret":true}}},{"title":"Username and Password","type":"object","required":["password"],"order":2,"properties":{"auth_type":{"type":"string","const":"Username and Password","enum":["Username and Password"],"default":"Username and Password","order":0},"password":{"description":"Enter the password associated with the username.","type":"string","airbyte_secret":true,"title":"Password","order":1}}}],"order":6},"jdbc_url_params":{"description":"Enter the additional properties to pass to the JDBC URL string when connecting to the database (formatted as key=value pairs separated by the symbol &). Example: key1=value1&key2=value2&key3=value3","title":"JDBC URL Params","type":"string","order":7},"raw_data_schema":{"type":"string","description":"The schema to write raw tables into","title":"Destinations V2 Raw Table Schema","order":10}}},"destination-databricks":{"title":"Databricks Lakehouse Destination Spec","type":"object","required":["accept_terms","databricks_server_hostname","databricks_http_path","databricks_personal_access_token","data_source","destinationType"],"properties":{"accept_terms":{"title":"Agree to the Databricks JDBC Driver Terms & Conditions","type":"boolean","description":"You must agree to the Databricks JDBC Driver <a href=\"https://databricks.com/jdbc-odbc-driver-license\">Terms & Conditions</a> to use this connector.","default":false,"order":1},"databricks_server_hostname":{"title":"Server Hostname","type":"string","description":"Databricks Cluster Server Hostname.","examples":["abc-12345678-wxyz.cloud.databricks.com"],"order":2},"databricks_http_path":{"title":"HTTP Path","type":"string","description":"Databricks Cluster HTTP Path.","examples":["sql/protocolvx/o/1234567489/0000-1111111-abcd90"],"order":3},"databricks_port":{"title":"Port","type":"string","description":"Databricks Cluster Port.","default":"443","examples":["443"],"order":4},"databricks_personal_access_token":{"title":"Access Token","type":"string","description":"Databricks Personal Access Token for making authenticated requests.","examples":["dapi0123456789abcdefghij0123456789AB"],"airbyte_secret":true,"order":5},"database":{"title":"Databricks catalog","description":"The name of the catalog. If not specified otherwise, the \"hive_metastore\" will be used.","type":"string","order":6},"schema":{"title":"Default Schema","description":"The default schema tables are written. If not specified otherwise, the \"default\" will be used.","type":"string","examples":["default"],"default":"default","order":7},"enable_schema_evolution":{"title":"Support schema evolution for all streams.","type":"boolean","description":"Support schema evolution for all streams. If \"false\", the connector might fail when a stream's schema changes.","default":false,"order":8},"data_source":{"title":"Data Source","type":"object","description":"Storage on which the delta lake is built.","default":"MANAGED_TABLES_STORAGE","order":9,"oneOf":[{"title":"[Recommended] Managed tables","required":["data_source_type"],"properties":{"data_source_type":{"type":"string","const":"MANAGED_TABLES_STORAGE","order":0,"enum":["MANAGED_TABLES_STORAGE"]}}},{"title":"Amazon S3","required":["data_source_type","s3_bucket_name","s3_bucket_path","s3_bucket_region","s3_access_key_id","s3_secret_access_key"],"properties":{"data_source_type":{"type":"string","const":"S3_STORAGE","order":1,"enum":["S3_STORAGE"]},"s3_bucket_name":{"title":"S3 Bucket Name","type":"string","description":"The name of the S3 bucket to use for intermittent staging of the data.","examples":["airbyte.staging"],"order":2},"s3_bucket_path":{"title":"S3 Bucket Path","type":"string","description":"The directory under the S3 bucket where data will be written.","examples":["data_sync/test"],"order":3},"s3_bucket_region":{"title":"S3 Bucket Region","type":"string","default":"","description":"The region of the S3 staging bucket to use if utilising a copy strategy.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1","us-gov-east-1","us-gov-west-1"],"order":4},"s3_access_key_id":{"type":"string","description":"The Access Key Id granting allow one to access the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket.","title":"S3 Access Key ID","examples":["A012345678910EXAMPLE"],"airbyte_secret":true,"order":5},"s3_secret_access_key":{"title":"S3 Secret Access Key","type":"string","description":"The corresponding secret to the above access key id.","examples":["a012345678910ABCDEFGH/AbCdEfGhEXAMPLEKEY"],"airbyte_secret":true,"order":6},"file_name_pattern":{"type":"string","description":"The pattern allows you to set the file-name format for the S3 staging file(s)","title":"S3 Filename pattern","examples":["{date}","{date:yyyy_MM}","{timestamp}","{part_number}","{sync_id}"],"order":7}}},{"title":"Azure Blob Storage","required":["data_source_type","azure_blob_storage_account_name","azure_blob_storage_container_name","azure_blob_storage_sas_token"],"properties":{"data_source_type":{"type":"string","const":"AZURE_BLOB_STORAGE","order":0,"enum":["AZURE_BLOB_STORAGE"]},"azure_blob_storage_endpoint_domain_name":{"title":"Endpoint Domain Name","type":"string","default":"blob.core.windows.net","description":"This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native from example.","examples":["blob.core.windows.net"],"order":1},"azure_blob_storage_account_name":{"title":"Azure Blob Storage Account Name","type":"string","description":"The account's name of the Azure Blob Storage.","examples":["airbyte5storage"],"order":2},"azure_blob_storage_container_name":{"title":"Azure Blob Storage Container Name","type":"string","description":"The name of the Azure blob storage container.","examples":["airbytetestcontainername"],"order":3},"azure_blob_storage_sas_token":{"title":"SAS Token","type":"string","airbyte_secret":true,"description":"Shared access signature (SAS) token to grant limited access to objects in your storage account.","examples":["?sv=2016-05-31&ss=b&srt=sco&sp=rwdl&se=2018-06-27T10:05:50Z&st=2017-06-27T02:05:50Z&spr=https,http&sig=bgqQwoXwxzuD2GJfagRg7VOS8hzNr3QLT7rhS8OFRLQ%3D"],"order":4}}}]},"purge_staging_data":{"title":"Purge Staging Files and Tables","type":"boolean","description":"Default to 'true'. Switch it to 'false' for debugging purpose.","default":true,"order":10},"destinationType":{"title":"databricks","const":"databricks","enum":["databricks"],"order":0,"type":"string"}}},"destination-databricks-update":{"title":"Databricks Lakehouse Destination Spec","type":"object","required":["accept_terms","databricks_server_hostname","databricks_http_path","databricks_personal_access_token","data_source"],"properties":{"accept_terms":{"title":"Agree to the Databricks JDBC Driver Terms & Conditions","type":"boolean","description":"You must agree to the Databricks JDBC Driver <a href=\"https://databricks.com/jdbc-odbc-driver-license\">Terms & Conditions</a> to use this connector.","default":false,"order":1},"databricks_server_hostname":{"title":"Server Hostname","type":"string","description":"Databricks Cluster Server Hostname.","examples":["abc-12345678-wxyz.cloud.databricks.com"],"order":2},"databricks_http_path":{"title":"HTTP Path","type":"string","description":"Databricks Cluster HTTP Path.","examples":["sql/protocolvx/o/1234567489/0000-1111111-abcd90"],"order":3},"databricks_port":{"title":"Port","type":"string","description":"Databricks Cluster Port.","default":"443","examples":["443"],"order":4},"databricks_personal_access_token":{"title":"Access Token","type":"string","description":"Databricks Personal Access Token for making authenticated requests.","examples":["dapi0123456789abcdefghij0123456789AB"],"airbyte_secret":true,"order":5},"database":{"title":"Databricks catalog","description":"The name of the catalog. If not specified otherwise, the \"hive_metastore\" will be used.","type":"string","order":6},"schema":{"title":"Default Schema","description":"The default schema tables are written. If not specified otherwise, the \"default\" will be used.","type":"string","examples":["default"],"default":"default","order":7},"enable_schema_evolution":{"title":"Support schema evolution for all streams.","type":"boolean","description":"Support schema evolution for all streams. If \"false\", the connector might fail when a stream's schema changes.","default":false,"order":8},"data_source":{"title":"Data Source","type":"object","description":"Storage on which the delta lake is built.","default":"MANAGED_TABLES_STORAGE","order":9,"oneOf":[{"title":"[Recommended] Managed tables","required":["data_source_type"],"properties":{"data_source_type":{"type":"string","const":"MANAGED_TABLES_STORAGE","order":0,"enum":["MANAGED_TABLES_STORAGE"]}}},{"title":"Amazon S3","required":["data_source_type","s3_bucket_name","s3_bucket_path","s3_bucket_region","s3_access_key_id","s3_secret_access_key"],"properties":{"data_source_type":{"type":"string","const":"S3_STORAGE","order":1,"enum":["S3_STORAGE"]},"s3_bucket_name":{"title":"S3 Bucket Name","type":"string","description":"The name of the S3 bucket to use for intermittent staging of the data.","examples":["airbyte.staging"],"order":2},"s3_bucket_path":{"title":"S3 Bucket Path","type":"string","description":"The directory under the S3 bucket where data will be written.","examples":["data_sync/test"],"order":3},"s3_bucket_region":{"title":"S3 Bucket Region","type":"string","default":"","description":"The region of the S3 staging bucket to use if utilising a copy strategy.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1","us-gov-east-1","us-gov-west-1"],"order":4},"s3_access_key_id":{"type":"string","description":"The Access Key Id granting allow one to access the above S3 staging bucket. Airbyte requires Read and Write permissions to the given bucket.","title":"S3 Access Key ID","examples":["A012345678910EXAMPLE"],"airbyte_secret":true,"order":5},"s3_secret_access_key":{"title":"S3 Secret Access Key","type":"string","description":"The corresponding secret to the above access key id.","examples":["a012345678910ABCDEFGH/AbCdEfGhEXAMPLEKEY"],"airbyte_secret":true,"order":6},"file_name_pattern":{"type":"string","description":"The pattern allows you to set the file-name format for the S3 staging file(s)","title":"S3 Filename pattern","examples":["{date}","{date:yyyy_MM}","{timestamp}","{part_number}","{sync_id}"],"order":7}}},{"title":"Azure Blob Storage","required":["data_source_type","azure_blob_storage_account_name","azure_blob_storage_container_name","azure_blob_storage_sas_token"],"properties":{"data_source_type":{"type":"string","const":"AZURE_BLOB_STORAGE","order":0,"enum":["AZURE_BLOB_STORAGE"]},"azure_blob_storage_endpoint_domain_name":{"title":"Endpoint Domain Name","type":"string","default":"blob.core.windows.net","description":"This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native from example.","examples":["blob.core.windows.net"],"order":1},"azure_blob_storage_account_name":{"title":"Azure Blob Storage Account Name","type":"string","description":"The account's name of the Azure Blob Storage.","examples":["airbyte5storage"],"order":2},"azure_blob_storage_container_name":{"title":"Azure Blob Storage Container Name","type":"string","description":"The name of the Azure blob storage container.","examples":["airbytetestcontainername"],"order":3},"azure_blob_storage_sas_token":{"title":"SAS Token","type":"string","airbyte_secret":true,"description":"Shared access signature (SAS) token to grant limited access to objects in your storage account.","examples":["?sv=2016-05-31&ss=b&srt=sco&sp=rwdl&se=2018-06-27T10:05:50Z&st=2017-06-27T02:05:50Z&spr=https,http&sig=bgqQwoXwxzuD2GJfagRg7VOS8hzNr3QLT7rhS8OFRLQ%3D"],"order":4}}}]},"purge_staging_data":{"title":"Purge Staging Files and Tables","type":"boolean","description":"Default to 'true'. Switch it to 'false' for debugging purpose.","default":true,"order":10}}},"destination-oracle":{"title":"Oracle Destination Spec","type":"object","required":["host","port","username","sid","destinationType"],"properties":{"host":{"title":"Host","description":"The hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"The port of the database.","type":"integer","minimum":0,"maximum":65536,"default":1521,"examples":["1521"],"order":1},"sid":{"title":"SID","description":"The System Identifier uniquely distinguishes the instance from any other instance on the same computer.","type":"string","order":2},"username":{"title":"User","description":"The username to access the database. This user must have CREATE USER privileges in the database.","type":"string","order":3},"password":{"title":"Password","description":"The password associated with the username.","type":"string","airbyte_secret":true,"order":4},"jdbc_url_params":{"description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","title":"JDBC URL Params","type":"string","order":5},"schema":{"title":"Default Schema","description":"The default schema is used as the target schema for all statements issued from the connection that do not explicitly specify a schema name. The usual value for this field is \"airbyte\".  In Oracle, schemas and users are the same thing, so the \"user\" parameter is used as the login credentials and this is used for the default Airbyte message schema.","type":"string","examples":["airbyte"],"default":"airbyte","order":6},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]},"destinationType":{"title":"oracle","const":"oracle","enum":["oracle"],"order":0,"type":"string"}}},"destination-oracle-update":{"title":"Oracle Destination Spec","type":"object","required":["host","port","username","sid"],"properties":{"host":{"title":"Host","description":"The hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"The port of the database.","type":"integer","minimum":0,"maximum":65536,"default":1521,"examples":["1521"],"order":1},"sid":{"title":"SID","description":"The System Identifier uniquely distinguishes the instance from any other instance on the same computer.","type":"string","order":2},"username":{"title":"User","description":"The username to access the database. This user must have CREATE USER privileges in the database.","type":"string","order":3},"password":{"title":"Password","description":"The password associated with the username.","type":"string","airbyte_secret":true,"order":4},"jdbc_url_params":{"description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","title":"JDBC URL Params","type":"string","order":5},"schema":{"title":"Default Schema","description":"The default schema is used as the target schema for all statements issued from the connection that do not explicitly specify a schema name. The usual value for this field is \"airbyte\".  In Oracle, schemas and users are the same thing, so the \"user\" parameter is used as the login credentials and this is used for the default Airbyte message schema.","type":"string","examples":["airbyte"],"default":"airbyte","order":6},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]}}},"destination-aws-datalake":{"title":"AWS Datalake Destination Spec","type":"object","required":["credentials","region","bucket_name","lakeformation_database_name","destinationType"],"properties":{"aws_account_id":{"type":"string","title":"AWS Account Id","description":"target aws account id","examples":["111111111111"],"order":1},"credentials":{"title":"Authentication mode","description":"Choose How to Authenticate to AWS.","type":"object","oneOf":[{"type":"object","title":"IAM Role","required":["role_arn","credentials_title"],"properties":{"credentials_title":{"type":"string","title":"Credentials Title","description":"Name of the credentials","const":"IAM Role","enum":["IAM Role"],"default":"IAM Role","order":0},"role_arn":{"title":"Target Role Arn","type":"string","description":"Will assume this role to write data to s3","airbyte_secret":false}}},{"type":"object","title":"IAM User","required":["credentials_title","aws_access_key_id","aws_secret_access_key"],"properties":{"credentials_title":{"type":"string","title":"Credentials Title","description":"Name of the credentials","const":"IAM User","enum":["IAM User"],"default":"IAM User","order":0},"aws_access_key_id":{"title":"Access Key Id","type":"string","description":"AWS User Access Key Id","airbyte_secret":true},"aws_secret_access_key":{"title":"Secret Access Key","type":"string","description":"Secret Access Key","airbyte_secret":true}}}],"order":2},"region":{"title":"S3 Bucket Region","type":"string","default":"","description":"The region of the S3 bucket. See <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions\">here</a> for all region codes.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1","us-gov-east-1","us-gov-west-1"],"order":3},"bucket_name":{"title":"S3 Bucket Name","type":"string","description":"The name of the S3 bucket. Read more <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html\">here</a>.","order":4},"bucket_prefix":{"title":"Target S3 Bucket Prefix","type":"string","description":"S3 prefix","order":5},"lakeformation_database_name":{"title":"Lake Formation Database Name","type":"string","description":"The default database this destination will use to create tables in per stream. Can be changed per connection by customizing the namespace.","order":6},"lakeformation_database_default_tag_key":{"title":"Lake Formation Database Tag Key","description":"Add a default tag key to databases created by this destination","examples":["pii_level"],"type":"string","order":7},"lakeformation_database_default_tag_values":{"title":"Lake Formation Database Tag Values","description":"Add default values for the `Tag Key` to databases created by this destination. Comma separate for multiple values.","examples":["private,public"],"type":"string","order":8},"lakeformation_governed_tables":{"title":"Lake Formation Governed Tables","description":"Whether to create tables as LF governed tables.","type":"boolean","default":false,"order":9},"format":{"title":"Output Format *","type":"object","description":"Format of the data output.","oneOf":[{"title":"JSON Lines: Newline-delimited JSON","required":["format_type"],"properties":{"format_type":{"title":"Format Type *","type":"string","enum":["JSONL"],"default":"JSONL"},"compression_codec":{"title":"Compression Codec (Optional)","description":"The compression algorithm used to compress data.","type":"string","enum":["UNCOMPRESSED","GZIP"],"default":"UNCOMPRESSED"}}},{"title":"Parquet: Columnar Storage","required":["format_type"],"properties":{"format_type":{"title":"Format Type *","type":"string","enum":["Parquet"],"default":"Parquet"},"compression_codec":{"title":"Compression Codec (Optional)","description":"The compression algorithm used to compress data.","type":"string","enum":["UNCOMPRESSED","SNAPPY","GZIP","ZSTD"],"default":"SNAPPY"}}}],"order":10},"partitioning":{"title":"Choose how to partition data","description":"Partition data by cursor fields when a cursor field is a date","type":"string","enum":["NO PARTITIONING","DATE","YEAR","MONTH","DAY","YEAR/MONTH","YEAR/MONTH/DAY"],"default":"NO PARTITIONING","order":11},"glue_catalog_float_as_decimal":{"title":"Glue Catalog: Float as Decimal","description":"Cast float/double as decimal(38,18). This can help achieve higher accuracy and represent numbers correctly as received from the source.","type":"boolean","default":false,"order":12},"destinationType":{"title":"aws-datalake","const":"aws-datalake","enum":["aws-datalake"],"order":0,"type":"string"}}},"destination-aws-datalake-update":{"title":"AWS Datalake Destination Spec","type":"object","required":["credentials","region","bucket_name","lakeformation_database_name"],"properties":{"aws_account_id":{"type":"string","title":"AWS Account Id","description":"target aws account id","examples":["111111111111"],"order":1},"credentials":{"title":"Authentication mode","description":"Choose How to Authenticate to AWS.","type":"object","oneOf":[{"type":"object","title":"IAM Role","required":["role_arn","credentials_title"],"properties":{"credentials_title":{"type":"string","title":"Credentials Title","description":"Name of the credentials","const":"IAM Role","enum":["IAM Role"],"default":"IAM Role","order":0},"role_arn":{"title":"Target Role Arn","type":"string","description":"Will assume this role to write data to s3","airbyte_secret":false}}},{"type":"object","title":"IAM User","required":["credentials_title","aws_access_key_id","aws_secret_access_key"],"properties":{"credentials_title":{"type":"string","title":"Credentials Title","description":"Name of the credentials","const":"IAM User","enum":["IAM User"],"default":"IAM User","order":0},"aws_access_key_id":{"title":"Access Key Id","type":"string","description":"AWS User Access Key Id","airbyte_secret":true},"aws_secret_access_key":{"title":"Secret Access Key","type":"string","description":"Secret Access Key","airbyte_secret":true}}}],"order":2},"region":{"title":"S3 Bucket Region","type":"string","default":"","description":"The region of the S3 bucket. See <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions\">here</a> for all region codes.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1","us-gov-east-1","us-gov-west-1"],"order":3},"bucket_name":{"title":"S3 Bucket Name","type":"string","description":"The name of the S3 bucket. Read more <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html\">here</a>.","order":4},"bucket_prefix":{"title":"Target S3 Bucket Prefix","type":"string","description":"S3 prefix","order":5},"lakeformation_database_name":{"title":"Lake Formation Database Name","type":"string","description":"The default database this destination will use to create tables in per stream. Can be changed per connection by customizing the namespace.","order":6},"lakeformation_database_default_tag_key":{"title":"Lake Formation Database Tag Key","description":"Add a default tag key to databases created by this destination","examples":["pii_level"],"type":"string","order":7},"lakeformation_database_default_tag_values":{"title":"Lake Formation Database Tag Values","description":"Add default values for the `Tag Key` to databases created by this destination. Comma separate for multiple values.","examples":["private,public"],"type":"string","order":8},"lakeformation_governed_tables":{"title":"Lake Formation Governed Tables","description":"Whether to create tables as LF governed tables.","type":"boolean","default":false,"order":9},"format":{"title":"Output Format *","type":"object","description":"Format of the data output.","oneOf":[{"title":"JSON Lines: Newline-delimited JSON","required":["format_type"],"properties":{"format_type":{"title":"Format Type *","type":"string","enum":["JSONL"],"default":"JSONL"},"compression_codec":{"title":"Compression Codec (Optional)","description":"The compression algorithm used to compress data.","type":"string","enum":["UNCOMPRESSED","GZIP"],"default":"UNCOMPRESSED"}}},{"title":"Parquet: Columnar Storage","required":["format_type"],"properties":{"format_type":{"title":"Format Type *","type":"string","enum":["Parquet"],"default":"Parquet"},"compression_codec":{"title":"Compression Codec (Optional)","description":"The compression algorithm used to compress data.","type":"string","enum":["UNCOMPRESSED","SNAPPY","GZIP","ZSTD"],"default":"SNAPPY"}}}],"order":10},"partitioning":{"title":"Choose how to partition data","description":"Partition data by cursor fields when a cursor field is a date","type":"string","enum":["NO PARTITIONING","DATE","YEAR","MONTH","DAY","YEAR/MONTH","YEAR/MONTH/DAY"],"default":"NO PARTITIONING","order":11},"glue_catalog_float_as_decimal":{"title":"Glue Catalog: Float as Decimal","description":"Cast float/double as decimal(38,18). This can help achieve higher accuracy and represent numbers correctly as received from the source.","type":"boolean","default":false,"order":12}}},"destination-milvus":{"title":"Milvus Destination Config","type":"object","properties":{"processing":{"title":"ProcessingConfigModel","type":"object","properties":{"chunk_size":{"title":"Chunk size","description":"Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)","maximum":8191,"type":"integer"},"chunk_overlap":{"title":"Chunk overlap","description":"Size of overlap between chunks in tokens to store in vector store to better capture relevant context","default":0,"type":"integer"},"text_fields":{"title":"Text fields to embed","description":"List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.","default":[],"always_show":true,"examples":["text","user.name","users.*.name"],"type":"array","items":{"type":"string"}},"metadata_fields":{"title":"Fields to store as metadata","description":"List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.","default":[],"always_show":true,"examples":["age","user","user.name"],"type":"array","items":{"type":"string"}}},"required":["chunk_size"],"group":"processing"},"embedding":{"title":"Embedding","description":"Embedding configuration","group":"embedding","type":"object","oneOf":[{"title":"OpenAI","type":"object","properties":{"mode":{"title":"Mode","default":"openai","const":"openai","enum":["openai"],"type":"string"},"openai_key":{"title":"OpenAI API key","airbyte_secret":true,"type":"string"}},"required":["openai_key"],"description":"Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions."},{"title":"Cohere","type":"object","properties":{"mode":{"title":"Mode","default":"cohere","const":"cohere","enum":["cohere"],"type":"string"},"cohere_key":{"title":"Cohere API key","airbyte_secret":true,"type":"string"}},"required":["cohere_key"],"description":"Use the Cohere API to embed text."},{"title":"Fake","type":"object","properties":{"mode":{"title":"Mode","default":"fake","const":"fake","enum":["fake"],"type":"string"}},"description":"Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs."},{"title":"From Field","type":"object","properties":{"mode":{"title":"Mode","default":"from_field","const":"from_field","enum":["from_field"],"type":"string"},"field_name":{"title":"Field name","description":"Name of the field in the record that contains the embedding","examples":["embedding","vector"],"type":"string"},"dimensions":{"title":"Embedding dimensions","description":"The number of dimensions the embedding model is generating","examples":[1536,384],"type":"integer"}},"required":["field_name","dimensions"],"description":"Use a field in the record as the embedding. This is useful if you already have an embedding for your data and want to store it in the vector store."}]},"indexing":{"title":"Indexing","type":"object","properties":{"host":{"title":"Public Endpoint","description":"The public endpoint of the Milvus instance. ","order":1,"examples":["https://my-instance.zone.zillizcloud.com","tcp://host.docker.internal:19530","tcp://my-local-milvus:19530"],"type":"string"},"db":{"title":"Database Name","description":"The database to connect to","default":"","type":"string"},"collection":{"title":"Collection Name","description":"The collection to load data into","order":3,"type":"string"},"auth":{"title":"Authentication","description":"Authentication method","type":"object","order":2,"oneOf":[{"title":"API Token","type":"object","properties":{"mode":{"title":"Mode","default":"token","const":"token","enum":["token"],"type":"string"},"token":{"title":"API Token","description":"API Token for the Milvus instance","airbyte_secret":true,"type":"string"}},"required":["token"],"description":"Authenticate using an API token (suitable for Zilliz Cloud)"},{"title":"Username/Password","type":"object","properties":{"mode":{"title":"Mode","default":"username_password","const":"username_password","enum":["username_password"],"type":"string"},"username":{"title":"Username","description":"Username for the Milvus instance","order":1,"type":"string"},"password":{"title":"Password","description":"Password for the Milvus instance","airbyte_secret":true,"order":2,"type":"string"}},"required":["username","password"],"description":"Authenticate using username and password (suitable for self-managed Milvus clusters)"},{"title":"No auth","type":"object","properties":{"mode":{"title":"Mode","default":"no_auth","const":"no_auth","enum":["no_auth"],"type":"string"}},"description":"Do not authenticate (suitable for locally running test clusters, do not use for clusters with public IP addresses)"}]},"vector_field":{"title":"Vector Field","description":"The field in the entity that contains the vector","default":"vector","type":"string"},"text_field":{"title":"Text Field","description":"The field in the entity that contains the embedded text","default":"text","type":"string"}},"required":["host","collection","auth"],"group":"indexing","description":"Indexing configuration"},"destinationType":{"title":"milvus","const":"milvus","enum":["milvus"],"order":0,"type":"string"}},"required":["processing","embedding","indexing","destinationType"],"groups":[{"id":"processing","title":"Processing"},{"id":"embedding","title":"Embedding"},{"id":"indexing","title":"Indexing"}]},"destination-milvus-update":{"title":"Milvus Destination Config","type":"object","properties":{"processing":{"title":"ProcessingConfigModel","type":"object","properties":{"chunk_size":{"title":"Chunk size","description":"Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)","maximum":8191,"type":"integer"},"chunk_overlap":{"title":"Chunk overlap","description":"Size of overlap between chunks in tokens to store in vector store to better capture relevant context","default":0,"type":"integer"},"text_fields":{"title":"Text fields to embed","description":"List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.","default":[],"always_show":true,"examples":["text","user.name","users.*.name"],"type":"array","items":{"type":"string"}},"metadata_fields":{"title":"Fields to store as metadata","description":"List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.","default":[],"always_show":true,"examples":["age","user","user.name"],"type":"array","items":{"type":"string"}}},"required":["chunk_size"],"group":"processing"},"embedding":{"title":"Embedding","description":"Embedding configuration","group":"embedding","type":"object","oneOf":[{"title":"OpenAI","type":"object","properties":{"mode":{"title":"Mode","default":"openai","const":"openai","enum":["openai"],"type":"string"},"openai_key":{"title":"OpenAI API key","airbyte_secret":true,"type":"string"}},"required":["openai_key"],"description":"Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions."},{"title":"Cohere","type":"object","properties":{"mode":{"title":"Mode","default":"cohere","const":"cohere","enum":["cohere"],"type":"string"},"cohere_key":{"title":"Cohere API key","airbyte_secret":true,"type":"string"}},"required":["cohere_key"],"description":"Use the Cohere API to embed text."},{"title":"Fake","type":"object","properties":{"mode":{"title":"Mode","default":"fake","const":"fake","enum":["fake"],"type":"string"}},"description":"Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs."},{"title":"From Field","type":"object","properties":{"mode":{"title":"Mode","default":"from_field","const":"from_field","enum":["from_field"],"type":"string"},"field_name":{"title":"Field name","description":"Name of the field in the record that contains the embedding","examples":["embedding","vector"],"type":"string"},"dimensions":{"title":"Embedding dimensions","description":"The number of dimensions the embedding model is generating","examples":[1536,384],"type":"integer"}},"required":["field_name","dimensions"],"description":"Use a field in the record as the embedding. This is useful if you already have an embedding for your data and want to store it in the vector store."}]},"indexing":{"title":"Indexing","type":"object","properties":{"host":{"title":"Public Endpoint","description":"The public endpoint of the Milvus instance. ","order":1,"examples":["https://my-instance.zone.zillizcloud.com","tcp://host.docker.internal:19530","tcp://my-local-milvus:19530"],"type":"string"},"db":{"title":"Database Name","description":"The database to connect to","default":"","type":"string"},"collection":{"title":"Collection Name","description":"The collection to load data into","order":3,"type":"string"},"auth":{"title":"Authentication","description":"Authentication method","type":"object","order":2,"oneOf":[{"title":"API Token","type":"object","properties":{"mode":{"title":"Mode","default":"token","const":"token","enum":["token"],"type":"string"},"token":{"title":"API Token","description":"API Token for the Milvus instance","airbyte_secret":true,"type":"string"}},"required":["token"],"description":"Authenticate using an API token (suitable for Zilliz Cloud)"},{"title":"Username/Password","type":"object","properties":{"mode":{"title":"Mode","default":"username_password","const":"username_password","enum":["username_password"],"type":"string"},"username":{"title":"Username","description":"Username for the Milvus instance","order":1,"type":"string"},"password":{"title":"Password","description":"Password for the Milvus instance","airbyte_secret":true,"order":2,"type":"string"}},"required":["username","password"],"description":"Authenticate using username and password (suitable for self-managed Milvus clusters)"},{"title":"No auth","type":"object","properties":{"mode":{"title":"Mode","default":"no_auth","const":"no_auth","enum":["no_auth"],"type":"string"}},"description":"Do not authenticate (suitable for locally running test clusters, do not use for clusters with public IP addresses)"}]},"vector_field":{"title":"Vector Field","description":"The field in the entity that contains the vector","default":"vector","type":"string"},"text_field":{"title":"Text Field","description":"The field in the entity that contains the embedded text","default":"text","type":"string"}},"required":["host","collection","auth"],"group":"indexing","description":"Indexing configuration"}},"required":["processing","embedding","indexing"],"groups":[{"id":"processing","title":"Processing"},{"id":"embedding","title":"Embedding"},{"id":"indexing","title":"Indexing"}]},"destination-firebolt":{"title":"Firebolt Spec","type":"object","required":["username","password","database","destinationType"],"properties":{"username":{"type":"string","title":"Username","description":"Firebolt email address you use to login.","examples":["username@email.com"],"order":0},"password":{"type":"string","title":"Password","description":"Firebolt password.","airbyte_secret":true,"order":1},"account":{"type":"string","title":"Account","description":"Firebolt account to login."},"host":{"type":"string","title":"Host","description":"The host name of your Firebolt database.","examples":["api.app.firebolt.io"]},"database":{"type":"string","title":"Database","description":"The database to connect to."},"engine":{"type":"string","title":"Engine","description":"Engine name or url to connect to."},"loading_method":{"type":"object","title":"Loading Method","description":"Loading method used to select the way data will be uploaded to Firebolt","oneOf":[{"title":"SQL Inserts","additionalProperties":false,"required":["method"],"properties":{"method":{"type":"string","const":"SQL","enum":["SQL"]}}},{"title":"External Table via S3","additionalProperties":false,"required":["method","s3_bucket","s3_region","aws_key_id","aws_key_secret"],"properties":{"method":{"type":"string","const":"S3","enum":["S3"]},"s3_bucket":{"type":"string","title":"S3 bucket name","description":"The name of the S3 bucket."},"s3_region":{"type":"string","title":"S3 region name","description":"Region name of the S3 bucket.","examples":["us-east-1"]},"aws_key_id":{"type":"string","title":"AWS Key ID","airbyte_secret":true,"description":"AWS access key granting read and write access to S3."},"aws_key_secret":{"type":"string","title":"AWS Key Secret","airbyte_secret":true,"description":"Corresponding secret part of the AWS Key"}}}]},"destinationType":{"title":"firebolt","const":"firebolt","enum":["firebolt"],"order":0,"type":"string"}}},"destination-firebolt-update":{"title":"Firebolt Spec","type":"object","required":["username","password","database"],"properties":{"username":{"type":"string","title":"Username","description":"Firebolt email address you use to login.","examples":["username@email.com"],"order":0},"password":{"type":"string","title":"Password","description":"Firebolt password.","airbyte_secret":true,"order":1},"account":{"type":"string","title":"Account","description":"Firebolt account to login."},"host":{"type":"string","title":"Host","description":"The host name of your Firebolt database.","examples":["api.app.firebolt.io"]},"database":{"type":"string","title":"Database","description":"The database to connect to."},"engine":{"type":"string","title":"Engine","description":"Engine name or url to connect to."},"loading_method":{"type":"object","title":"Loading Method","description":"Loading method used to select the way data will be uploaded to Firebolt","oneOf":[{"title":"SQL Inserts","additionalProperties":false,"required":["method"],"properties":{"method":{"type":"string","const":"SQL","enum":["SQL"]}}},{"title":"External Table via S3","additionalProperties":false,"required":["method","s3_bucket","s3_region","aws_key_id","aws_key_secret"],"properties":{"method":{"type":"string","const":"S3","enum":["S3"]},"s3_bucket":{"type":"string","title":"S3 bucket name","description":"The name of the S3 bucket."},"s3_region":{"type":"string","title":"S3 region name","description":"Region name of the S3 bucket.","examples":["us-east-1"]},"aws_key_id":{"type":"string","title":"AWS Key ID","airbyte_secret":true,"description":"AWS access key granting read and write access to S3."},"aws_key_secret":{"type":"string","title":"AWS Key Secret","airbyte_secret":true,"description":"Corresponding secret part of the AWS Key"}}}]}}},"destination-google-sheets":{"title":"Destination Google Sheets","type":"object","required":["spreadsheet_id","credentials","destinationType"],"properties":{"spreadsheet_id":{"type":"string","title":"Spreadsheet Link","description":"The link to your spreadsheet. See <a href='https://docs.airbyte.com/integrations/destinations/google-sheets#sheetlink'>this guide</a> for more details.","examples":["https://docs.google.com/spreadsheets/d/1hLd9Qqti3UyLXZB2aFfUWDT7BG/edit"]},"credentials":{"type":"object","title":"Authentication via Google (OAuth)","description":"Google API Credentials for connecting to Google Sheets and Google Drive APIs","required":["client_id","client_secret","refresh_token"],"properties":{"client_id":{"title":"Client ID","type":"string","description":"The Client ID of your Google Sheets developer application.","airbyte_secret":true},"client_secret":{"title":"Client Secret","type":"string","description":"The Client Secret of your Google Sheets developer application.","airbyte_secret":true},"refresh_token":{"title":"Refresh Token","type":"string","description":"The token for obtaining new access token.","airbyte_secret":true}}},"destinationType":{"title":"google-sheets","const":"google-sheets","enum":["google-sheets"],"order":0,"type":"string"}}},"destination-google-sheets-update":{"title":"Destination Google Sheets","type":"object","required":["spreadsheet_id","credentials"],"properties":{"spreadsheet_id":{"type":"string","title":"Spreadsheet Link","description":"The link to your spreadsheet. See <a href='https://docs.airbyte.com/integrations/destinations/google-sheets#sheetlink'>this guide</a> for more details.","examples":["https://docs.google.com/spreadsheets/d/1hLd9Qqti3UyLXZB2aFfUWDT7BG/edit"]},"credentials":{"type":"object","title":"Authentication via Google (OAuth)","description":"Google API Credentials for connecting to Google Sheets and Google Drive APIs","required":["client_id","client_secret","refresh_token"],"properties":{"client_id":{"title":"Client ID","type":"string","description":"The Client ID of your Google Sheets developer application.","airbyte_secret":true},"client_secret":{"title":"Client Secret","type":"string","description":"The Client Secret of your Google Sheets developer application.","airbyte_secret":true},"refresh_token":{"title":"Refresh Token","type":"string","description":"The token for obtaining new access token.","airbyte_secret":true}}}}},"destination-databend":{"title":"Destination Databend","type":"object","required":["host","username","database","destinationType"],"properties":{"host":{"title":"Host","description":"Hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"Port of the database.","type":"integer","minimum":0,"maximum":65536,"default":443,"examples":["443"],"order":2},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":3},"table":{"title":"Default Table","description":"The default  table was written to.","type":"string","examples":["default"],"default":"default","order":4},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":5},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":6},"destinationType":{"title":"databend","const":"databend","enum":["databend"],"order":0,"type":"string"}}},"destination-databend-update":{"title":"Destination Databend","type":"object","required":["host","username","database"],"properties":{"host":{"title":"Host","description":"Hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"Port of the database.","type":"integer","minimum":0,"maximum":65536,"default":443,"examples":["443"],"order":2},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":3},"table":{"title":"Default Table","description":"The default  table was written to.","type":"string","examples":["default"],"default":"default","order":4},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":5},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":6}}},"destination-pinecone":{"title":"Pinecone Destination Config","type":"object","properties":{"indexing":{"title":"Indexing","type":"object","properties":{"pinecone_key":{"title":"Pinecone API key","airbyte_secret":true,"type":"string"},"pinecone_environment":{"title":"Pinecone environment","description":"Pinecone environment to use","type":"string"},"index":{"title":"Index","description":"Pinecone index to use","type":"string"}},"required":["pinecone_key","pinecone_environment","index"],"description":"Pinecone is a popular vector store that can be used to store and retrieve embeddings.","group":"indexing"},"embedding":{"title":"Embedding","description":"Embedding configuration","group":"embedding","type":"object","oneOf":[{"title":"OpenAI","type":"object","properties":{"mode":{"title":"Mode","default":"openai","const":"openai","enum":["openai"],"type":"string"},"openai_key":{"title":"OpenAI API key","airbyte_secret":true,"type":"string"}},"required":["openai_key"],"description":"Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions."},{"title":"Cohere","type":"object","properties":{"mode":{"title":"Mode","default":"cohere","const":"cohere","enum":["cohere"],"type":"string"},"cohere_key":{"title":"Cohere API key","airbyte_secret":true,"type":"string"}},"required":["cohere_key"],"description":"Use the Cohere API to embed text."},{"title":"Fake","type":"object","properties":{"mode":{"title":"Mode","default":"fake","const":"fake","enum":["fake"],"type":"string"}},"description":"Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs."}]},"processing":{"title":"ProcessingConfigModel","type":"object","properties":{"chunk_size":{"title":"Chunk size","description":"Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)","maximum":8191,"type":"integer"},"chunk_overlap":{"title":"Chunk overlap","description":"Size of overlap between chunks in tokens to store in vector store to better capture relevant context","default":0,"type":"integer"},"text_fields":{"title":"Text fields to embed","description":"List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.","default":[],"always_show":true,"examples":["text","user.name","users.*.name"],"type":"array","items":{"type":"string"}},"metadata_fields":{"title":"Fields to store as metadata","description":"List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.","default":[],"always_show":true,"examples":["age","user","user.name"],"type":"array","items":{"type":"string"}}},"required":["chunk_size"],"group":"processing"},"destinationType":{"title":"pinecone","const":"pinecone","enum":["pinecone"],"order":0,"type":"string"}},"required":["indexing","embedding","processing","destinationType"],"groups":[{"id":"processing","title":"Processing"},{"id":"embedding","title":"Embedding"},{"id":"indexing","title":"Indexing"}]},"destination-pinecone-update":{"title":"Pinecone Destination Config","type":"object","properties":{"indexing":{"title":"Indexing","type":"object","properties":{"pinecone_key":{"title":"Pinecone API key","airbyte_secret":true,"type":"string"},"pinecone_environment":{"title":"Pinecone environment","description":"Pinecone environment to use","type":"string"},"index":{"title":"Index","description":"Pinecone index to use","type":"string"}},"required":["pinecone_key","pinecone_environment","index"],"description":"Pinecone is a popular vector store that can be used to store and retrieve embeddings.","group":"indexing"},"embedding":{"title":"Embedding","description":"Embedding configuration","group":"embedding","type":"object","oneOf":[{"title":"OpenAI","type":"object","properties":{"mode":{"title":"Mode","default":"openai","const":"openai","enum":["openai"],"type":"string"},"openai_key":{"title":"OpenAI API key","airbyte_secret":true,"type":"string"}},"required":["openai_key"],"description":"Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions."},{"title":"Cohere","type":"object","properties":{"mode":{"title":"Mode","default":"cohere","const":"cohere","enum":["cohere"],"type":"string"},"cohere_key":{"title":"Cohere API key","airbyte_secret":true,"type":"string"}},"required":["cohere_key"],"description":"Use the Cohere API to embed text."},{"title":"Fake","type":"object","properties":{"mode":{"title":"Mode","default":"fake","const":"fake","enum":["fake"],"type":"string"}},"description":"Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs."}]},"processing":{"title":"ProcessingConfigModel","type":"object","properties":{"chunk_size":{"title":"Chunk size","description":"Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)","maximum":8191,"type":"integer"},"chunk_overlap":{"title":"Chunk overlap","description":"Size of overlap between chunks in tokens to store in vector store to better capture relevant context","default":0,"type":"integer"},"text_fields":{"title":"Text fields to embed","description":"List of fields in the record that should be used to calculate the embedding. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.","default":[],"always_show":true,"examples":["text","user.name","users.*.name"],"type":"array","items":{"type":"string"}},"metadata_fields":{"title":"Fields to store as metadata","description":"List of fields in the record that should be stored as metadata. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered metadata fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array. When specifying nested paths, all matching values are flattened into an array set to a field named by the path.","default":[],"always_show":true,"examples":["age","user","user.name"],"type":"array","items":{"type":"string"}}},"required":["chunk_size"],"group":"processing"}},"required":["indexing","embedding","processing"],"groups":[{"id":"processing","title":"Processing"},{"id":"embedding","title":"Embedding"},{"id":"indexing","title":"Indexing"}]},"destination-bigquery-denormalized":{"title":"BigQuery Denormalized Typed Struct Destination Spec","type":"object","required":["project_id","dataset_id","destinationType"],"properties":{"project_id":{"type":"string","description":"The GCP project ID for the project containing the target BigQuery dataset. Read more <a href=\"https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects\">here</a>.","title":"Project ID","order":0},"dataset_id":{"type":"string","description":"The default BigQuery Dataset ID that tables are replicated to if the source does not specify a namespace. Read more <a href=\"https://cloud.google.com/bigquery/docs/datasets#create-dataset\">here</a>.","title":"Default Dataset ID","order":1},"loading_method":{"type":"object","title":"Loading Method","description":"Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href=\"https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging\">here</a>.","order":2,"oneOf":[{"title":"Standard Inserts","required":["method"],"properties":{"method":{"type":"string","const":"Standard","enum":["Standard"]}}},{"title":"GCS Staging","type":"object","required":["method","gcs_bucket_name","gcs_bucket_path","credential"],"properties":{"method":{"type":"string","const":"GCS Staging","order":0,"enum":["GCS Staging"]},"credential":{"title":"Credential","description":"An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href=\"https://cloud.google.com/storage/docs/authentication/hmackeys\">here</a>.","type":"object","order":1,"oneOf":[{"title":"HMAC key","order":0,"required":["credential_type","hmac_key_access_id","hmac_key_secret"],"properties":{"credential_type":{"type":"string","const":"HMAC_KEY","order":0,"enum":["HMAC_KEY"]},"hmac_key_access_id":{"type":"string","description":"HMAC key access ID. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long.","title":"HMAC Key Access ID","airbyte_secret":true,"examples":["1234567890abcdefghij1234"],"order":1},"hmac_key_secret":{"type":"string","description":"The corresponding secret for the access ID. It is a 40-character base-64 encoded string.","title":"HMAC Key Secret","airbyte_secret":true,"examples":["1234567890abcdefghij1234567890ABCDEFGHIJ"],"order":2}}}]},"gcs_bucket_name":{"title":"GCS Bucket Name","type":"string","description":"The name of the GCS bucket. Read more <a href=\"https://cloud.google.com/storage/docs/naming-buckets\">here</a>.","examples":["airbyte_sync"],"order":2},"gcs_bucket_path":{"title":"GCS Bucket Path","description":"Directory under the GCS bucket where data will be written. Read more <a href=\"https://cloud.google.com/storage/docs/locations\">here</a>.","type":"string","examples":["data_sync/test"],"order":3},"keep_files_in_gcs-bucket":{"type":"string","description":"This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default \"Delete all tmp files from GCS\" value is used if not set explicitly.","title":"GCS Tmp Files Afterward Processing","default":"Delete all tmp files from GCS","enum":["Delete all tmp files from GCS","Keep all tmp files in GCS"],"order":4},"file_buffer_count":{"title":"File Buffer Count","type":"integer","minimum":10,"maximum":50,"default":10,"description":"Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects","examples":["10"],"order":5}}}]},"credentials_json":{"type":"string","description":"The contents of the JSON service account key. Check out the <a href=\"https://docs.airbyte.com/integrations/destinations/bigquery#service-account-key\">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.","title":"Service Account Key JSON (Required for cloud, optional for open-source)","airbyte_secret":true,"order":3,"always_show":true},"dataset_location":{"type":"string","description":"The location of the dataset. Warning: Changes made after creation will not be applied. The default \"US\" value is used if not set explicitly. Read more <a href=\"https://cloud.google.com/bigquery/docs/locations\">here</a>.","title":"Dataset Location","default":"US","order":4,"enum":["US","EU","asia-east1","asia-east2","asia-northeast1","asia-northeast2","asia-northeast3","asia-south1","asia-south2","asia-southeast1","asia-southeast2","australia-southeast1","australia-southeast2","europe-central1","europe-central2","europe-north1","europe-southwest1","europe-west1","europe-west2","europe-west3","europe-west4","europe-west6","europe-west7","europe-west8","europe-west9","me-west1","northamerica-northeast1","northamerica-northeast2","southamerica-east1","southamerica-west1","us-central1","us-east1","us-east2","us-east3","us-east4","us-east5","us-west1","us-west2","us-west3","us-west4"]},"big_query_client_buffer_size_mb":{"title":"Google BigQuery Client Chunk Size","description":"Google BigQuery client's chunk (buffer) size (MIN=1, MAX = 15) for each table. The size that will be written by a single RPC. Written data will be buffered and only flushed upon reaching this size or closing the channel. The default 15MB value is used if not set explicitly. Read more <a href=\"https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html\">here</a>.","type":"integer","minimum":1,"maximum":15,"default":15,"examples":["15"],"order":5},"destinationType":{"title":"bigquery-denormalized","const":"bigquery-denormalized","enum":["bigquery-denormalized"],"order":0,"type":"string"}}},"destination-bigquery-denormalized-update":{"title":"BigQuery Denormalized Typed Struct Destination Spec","type":"object","required":["project_id","dataset_id"],"properties":{"project_id":{"type":"string","description":"The GCP project ID for the project containing the target BigQuery dataset. Read more <a href=\"https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects\">here</a>.","title":"Project ID","order":0},"dataset_id":{"type":"string","description":"The default BigQuery Dataset ID that tables are replicated to if the source does not specify a namespace. Read more <a href=\"https://cloud.google.com/bigquery/docs/datasets#create-dataset\">here</a>.","title":"Default Dataset ID","order":1},"loading_method":{"type":"object","title":"Loading Method","description":"Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href=\"https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging\">here</a>.","order":2,"oneOf":[{"title":"Standard Inserts","required":["method"],"properties":{"method":{"type":"string","const":"Standard","enum":["Standard"]}}},{"title":"GCS Staging","type":"object","required":["method","gcs_bucket_name","gcs_bucket_path","credential"],"properties":{"method":{"type":"string","const":"GCS Staging","order":0,"enum":["GCS Staging"]},"credential":{"title":"Credential","description":"An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href=\"https://cloud.google.com/storage/docs/authentication/hmackeys\">here</a>.","type":"object","order":1,"oneOf":[{"title":"HMAC key","order":0,"required":["credential_type","hmac_key_access_id","hmac_key_secret"],"properties":{"credential_type":{"type":"string","const":"HMAC_KEY","order":0,"enum":["HMAC_KEY"]},"hmac_key_access_id":{"type":"string","description":"HMAC key access ID. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long.","title":"HMAC Key Access ID","airbyte_secret":true,"examples":["1234567890abcdefghij1234"],"order":1},"hmac_key_secret":{"type":"string","description":"The corresponding secret for the access ID. It is a 40-character base-64 encoded string.","title":"HMAC Key Secret","airbyte_secret":true,"examples":["1234567890abcdefghij1234567890ABCDEFGHIJ"],"order":2}}}]},"gcs_bucket_name":{"title":"GCS Bucket Name","type":"string","description":"The name of the GCS bucket. Read more <a href=\"https://cloud.google.com/storage/docs/naming-buckets\">here</a>.","examples":["airbyte_sync"],"order":2},"gcs_bucket_path":{"title":"GCS Bucket Path","description":"Directory under the GCS bucket where data will be written. Read more <a href=\"https://cloud.google.com/storage/docs/locations\">here</a>.","type":"string","examples":["data_sync/test"],"order":3},"keep_files_in_gcs-bucket":{"type":"string","description":"This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default \"Delete all tmp files from GCS\" value is used if not set explicitly.","title":"GCS Tmp Files Afterward Processing","default":"Delete all tmp files from GCS","enum":["Delete all tmp files from GCS","Keep all tmp files in GCS"],"order":4},"file_buffer_count":{"title":"File Buffer Count","type":"integer","minimum":10,"maximum":50,"default":10,"description":"Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects","examples":["10"],"order":5}}}]},"credentials_json":{"type":"string","description":"The contents of the JSON service account key. Check out the <a href=\"https://docs.airbyte.com/integrations/destinations/bigquery#service-account-key\">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.","title":"Service Account Key JSON (Required for cloud, optional for open-source)","airbyte_secret":true,"order":3,"always_show":true},"dataset_location":{"type":"string","description":"The location of the dataset. Warning: Changes made after creation will not be applied. The default \"US\" value is used if not set explicitly. Read more <a href=\"https://cloud.google.com/bigquery/docs/locations\">here</a>.","title":"Dataset Location","default":"US","order":4,"enum":["US","EU","asia-east1","asia-east2","asia-northeast1","asia-northeast2","asia-northeast3","asia-south1","asia-south2","asia-southeast1","asia-southeast2","australia-southeast1","australia-southeast2","europe-central1","europe-central2","europe-north1","europe-southwest1","europe-west1","europe-west2","europe-west3","europe-west4","europe-west6","europe-west7","europe-west8","europe-west9","me-west1","northamerica-northeast1","northamerica-northeast2","southamerica-east1","southamerica-west1","us-central1","us-east1","us-east2","us-east3","us-east4","us-east5","us-west1","us-west2","us-west3","us-west4"]},"big_query_client_buffer_size_mb":{"title":"Google BigQuery Client Chunk Size","description":"Google BigQuery client's chunk (buffer) size (MIN=1, MAX = 15) for each table. The size that will be written by a single RPC. Written data will be buffered and only flushed upon reaching this size or closing the channel. The default 15MB value is used if not set explicitly. Read more <a href=\"https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html\">here</a>.","type":"integer","minimum":1,"maximum":15,"default":15,"examples":["15"],"order":5}}},"destination-sftp-json":{"title":"Destination SFTP JSON","type":"object","required":["host","username","password","destination_path","destinationType"],"properties":{"host":{"title":"Host","description":"Hostname of the SFTP server.","type":"string","order":0},"port":{"title":"Port","description":"Port of the SFTP server.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":[22],"order":1},"username":{"title":"User","description":"Username to use to access the SFTP server.","type":"string","order":2},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":3},"destination_path":{"title":"Destination path","type":"string","description":"Path to the directory where json files will be written.","examples":["/json_data"],"order":4},"destinationType":{"title":"sftp-json","const":"sftp-json","enum":["sftp-json"],"order":0,"type":"string"}}},"destination-sftp-json-update":{"title":"Destination SFTP JSON","type":"object","required":["host","username","password","destination_path"],"properties":{"host":{"title":"Host","description":"Hostname of the SFTP server.","type":"string","order":0},"port":{"title":"Port","description":"Port of the SFTP server.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":[22],"order":1},"username":{"title":"User","description":"Username to use to access the SFTP server.","type":"string","order":2},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":3},"destination_path":{"title":"Destination path","type":"string","description":"Path to the directory where json files will be written.","examples":["/json_data"],"order":4}}},"destination-kinesis":{"title":"Kinesis Destination Spec","type":"object","required":["endpoint","region","shardCount","accessKey","privateKey","bufferSize","destinationType"],"properties":{"endpoint":{"title":"Endpoint","description":"AWS Kinesis endpoint.","type":"string","examples":["kinesis.us‑west‑1.amazonaws.com"],"order":0},"region":{"title":"Region","description":"AWS region. Your account determines the Regions that are available to you.","type":"string","examples":["us‑west‑1"],"order":1},"shardCount":{"title":"Shard Count","description":"Number of shards to which the data should be streamed.","type":"integer","default":5,"order":2},"accessKey":{"title":"Access Key","description":"Generate the AWS Access Key for current user.","airbyte_secret":true,"type":"string","order":3},"privateKey":{"title":"Private Key","description":"The AWS Private Key - a string of numbers and letters that are unique for each account, also known as a \"recovery phrase\".","airbyte_secret":true,"type":"string","order":4},"bufferSize":{"title":"Buffer Size","description":"Buffer size for storing kinesis records before being batch streamed.","type":"integer","minimum":1,"maximum":500,"default":100,"order":5},"destinationType":{"title":"kinesis","const":"kinesis","enum":["kinesis"],"order":0,"type":"string"}}},"destination-kinesis-update":{"title":"Kinesis Destination Spec","type":"object","required":["endpoint","region","shardCount","accessKey","privateKey","bufferSize"],"properties":{"endpoint":{"title":"Endpoint","description":"AWS Kinesis endpoint.","type":"string","examples":["kinesis.us‑west‑1.amazonaws.com"],"order":0},"region":{"title":"Region","description":"AWS region. Your account determines the Regions that are available to you.","type":"string","examples":["us‑west‑1"],"order":1},"shardCount":{"title":"Shard Count","description":"Number of shards to which the data should be streamed.","type":"integer","default":5,"order":2},"accessKey":{"title":"Access Key","description":"Generate the AWS Access Key for current user.","airbyte_secret":true,"type":"string","order":3},"privateKey":{"title":"Private Key","description":"The AWS Private Key - a string of numbers and letters that are unique for each account, also known as a \"recovery phrase\".","airbyte_secret":true,"type":"string","order":4},"bufferSize":{"title":"Buffer Size","description":"Buffer size for storing kinesis records before being batch streamed.","type":"integer","minimum":1,"maximum":500,"default":100,"order":5}}},"destination-s3":{"title":"S3 Destination Spec","type":"object","required":["s3_bucket_name","s3_bucket_path","s3_bucket_region","format","destinationType"],"properties":{"access_key_id":{"type":"string","description":"The access key ID to access the S3 bucket. Airbyte requires Read and Write permissions to the given bucket. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">here</a>.","title":"S3 Key ID","airbyte_secret":true,"examples":["A012345678910EXAMPLE"],"order":0},"secret_access_key":{"type":"string","description":"The corresponding secret to the access key ID. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">here</a>","title":"S3 Access Key","airbyte_secret":true,"examples":["a012345678910ABCDEFGH/AbCdEfGhEXAMPLEKEY"],"order":1},"s3_bucket_name":{"title":"S3 Bucket Name","type":"string","description":"The name of the S3 bucket. Read more <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html\">here</a>.","examples":["airbyte_sync"],"order":2},"s3_bucket_path":{"title":"S3 Bucket Path","description":"Directory under the S3 bucket where data will be written. Read more <a href=\"https://docs.airbyte.com/integrations/destinations/s3#:~:text=to%20format%20the-,bucket%20path,-%3A\">here</a>","type":"string","examples":["data_sync/test"],"order":3},"s3_bucket_region":{"title":"S3 Bucket Region","type":"string","default":"","description":"The region of the S3 bucket. See <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions\">here</a> for all region codes.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1","us-gov-east-1","us-gov-west-1"],"order":4},"format":{"title":"Output Format","type":"object","description":"Format of the data output. See <a href=\"https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema\">here</a> for more details","oneOf":[{"title":"Avro: Apache Avro","required":["format_type","compression_codec"],"properties":{"format_type":{"title":"Format Type","type":"string","enum":["Avro"],"default":"Avro","order":0},"compression_codec":{"title":"Compression Codec","description":"The compression algorithm used to compress data. Default to no compression.","type":"object","oneOf":[{"title":"No Compression","required":["codec"],"properties":{"codec":{"type":"string","enum":["no compression"],"default":"no compression"}}},{"title":"Deflate","required":["codec","compression_level"],"properties":{"codec":{"type":"string","enum":["Deflate"],"default":"Deflate"},"compression_level":{"title":"Deflate Level","description":"0: no compression & fastest, 9: best compression & slowest.","type":"integer","default":0,"minimum":0,"maximum":9}}},{"title":"bzip2","required":["codec"],"properties":{"codec":{"type":"string","enum":["bzip2"],"default":"bzip2"}}},{"title":"xz","required":["codec","compression_level"],"properties":{"codec":{"type":"string","enum":["xz"],"default":"xz"},"compression_level":{"title":"Compression Level","description":"See <a href=\"https://commons.apache.org/proper/commons-compress/apidocs/org/apache/commons/compress/compressors/xz/XZCompressorOutputStream.html#XZCompressorOutputStream-java.io.OutputStream-int-\">here</a> for details.","type":"integer","default":6,"minimum":0,"maximum":9}}},{"title":"zstandard","required":["codec","compression_level"],"properties":{"codec":{"type":"string","enum":["zstandard"],"default":"zstandard"},"compression_level":{"title":"Compression Level","description":"Negative levels are 'fast' modes akin to lz4 or snappy, levels above 9 are generally for archival purposes, and levels above 18 use a lot of memory.","type":"integer","default":3,"minimum":-5,"maximum":22},"include_checksum":{"title":"Include Checksum","description":"If true, include a checksum with each data block.","type":"boolean","default":false}}},{"title":"snappy","required":["codec"],"properties":{"codec":{"type":"string","enum":["snappy"],"default":"snappy"}}}],"order":1}}},{"title":"CSV: Comma-Separated Values","required":["format_type","flattening"],"properties":{"format_type":{"title":"Format Type","type":"string","enum":["CSV"],"default":"CSV"},"flattening":{"type":"string","title":"Flattening","description":"Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details.","default":"No flattening","enum":["No flattening","Root level flattening"]},"compression":{"title":"Compression","type":"object","description":"Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \".csv.gz\").","oneOf":[{"title":"No Compression","requires":["compression_type"],"properties":{"compression_type":{"type":"string","enum":["No Compression"],"default":"No Compression"}}},{"title":"GZIP","requires":["compression_type"],"properties":{"compression_type":{"type":"string","enum":["GZIP"],"default":"GZIP"}}}]}}},{"title":"JSON Lines: Newline-delimited JSON","required":["format_type"],"properties":{"format_type":{"title":"Format Type","type":"string","enum":["JSONL"],"default":"JSONL"},"flattening":{"type":"string","title":"Flattening","description":"Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details.","default":"No flattening","enum":["No flattening","Root level flattening"]},"compression":{"title":"Compression","type":"object","description":"Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \".jsonl.gz\").","oneOf":[{"title":"No Compression","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["No Compression"],"default":"No Compression"}}},{"title":"GZIP","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["GZIP"],"default":"GZIP"}}}]}}},{"title":"Parquet: Columnar Storage","required":["format_type"],"properties":{"format_type":{"title":"Format Type","type":"string","enum":["Parquet"],"default":"Parquet"},"compression_codec":{"title":"Compression Codec","description":"The compression algorithm used to compress data pages.","type":"string","enum":["UNCOMPRESSED","SNAPPY","GZIP","LZO","BROTLI","LZ4","ZSTD"],"default":"UNCOMPRESSED"},"block_size_mb":{"title":"Block Size (Row Group Size) (MB)","description":"This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. Default: 128 MB.","type":"integer","default":128,"examples":[128]},"max_padding_size_mb":{"title":"Max Padding Size (MB)","description":"Maximum size allowed as padding to align row groups. This is also the minimum size of a row group. Default: 8 MB.","type":"integer","default":8,"examples":[8]},"page_size_kb":{"title":"Page Size (KB)","description":"The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. Default: 1024 KB.","type":"integer","default":1024,"examples":[1024]},"dictionary_page_size_kb":{"title":"Dictionary Page Size (KB)","description":"There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. Default: 1024 KB.","type":"integer","default":1024,"examples":[1024]},"dictionary_encoding":{"title":"Dictionary Encoding","description":"Default: true.","type":"boolean","default":true}}}],"order":5},"s3_endpoint":{"title":"Endpoint","type":"string","default":"","description":"Your S3 endpoint url. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html#:~:text=Service%20endpoints-,Amazon%20S3%20endpoints,-When%20you%20use\">here</a>","examples":["http://localhost:9000"],"order":6},"s3_path_format":{"title":"S3 Path Format","description":"Format string on how data will be organized inside the S3 bucket directory. Read more <a href=\"https://docs.airbyte.com/integrations/destinations/s3#:~:text=The%20full%20path%20of%20the%20output%20data%20with%20the%20default%20S3%20path%20format\">here</a>","type":"string","examples":["${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_"],"order":7},"file_name_pattern":{"type":"string","description":"The pattern allows you to set the file-name format for the S3 staging file(s)","title":"S3 Filename pattern","examples":["{date}","{date:yyyy_MM}","{timestamp}","{part_number}","{sync_id}"],"order":8},"destinationType":{"title":"s3","const":"s3","enum":["s3"],"order":0,"type":"string"}}},"destination-s3-update":{"title":"S3 Destination Spec","type":"object","required":["s3_bucket_name","s3_bucket_path","s3_bucket_region","format"],"properties":{"access_key_id":{"type":"string","description":"The access key ID to access the S3 bucket. Airbyte requires Read and Write permissions to the given bucket. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">here</a>.","title":"S3 Key ID","airbyte_secret":true,"examples":["A012345678910EXAMPLE"],"order":0},"secret_access_key":{"type":"string","description":"The corresponding secret to the access key ID. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys\">here</a>","title":"S3 Access Key","airbyte_secret":true,"examples":["a012345678910ABCDEFGH/AbCdEfGhEXAMPLEKEY"],"order":1},"s3_bucket_name":{"title":"S3 Bucket Name","type":"string","description":"The name of the S3 bucket. Read more <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html\">here</a>.","examples":["airbyte_sync"],"order":2},"s3_bucket_path":{"title":"S3 Bucket Path","description":"Directory under the S3 bucket where data will be written. Read more <a href=\"https://docs.airbyte.com/integrations/destinations/s3#:~:text=to%20format%20the-,bucket%20path,-%3A\">here</a>","type":"string","examples":["data_sync/test"],"order":3},"s3_bucket_region":{"title":"S3 Bucket Region","type":"string","default":"","description":"The region of the S3 bucket. See <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions\">here</a> for all region codes.","enum":["","us-east-1","us-east-2","us-west-1","us-west-2","af-south-1","ap-east-1","ap-south-1","ap-northeast-1","ap-northeast-2","ap-northeast-3","ap-southeast-1","ap-southeast-2","ca-central-1","cn-north-1","cn-northwest-1","eu-central-1","eu-north-1","eu-south-1","eu-west-1","eu-west-2","eu-west-3","sa-east-1","me-south-1","us-gov-east-1","us-gov-west-1"],"order":4},"format":{"title":"Output Format","type":"object","description":"Format of the data output. See <a href=\"https://docs.airbyte.com/integrations/destinations/s3/#supported-output-schema\">here</a> for more details","oneOf":[{"title":"Avro: Apache Avro","required":["format_type","compression_codec"],"properties":{"format_type":{"title":"Format Type","type":"string","enum":["Avro"],"default":"Avro","order":0},"compression_codec":{"title":"Compression Codec","description":"The compression algorithm used to compress data. Default to no compression.","type":"object","oneOf":[{"title":"No Compression","required":["codec"],"properties":{"codec":{"type":"string","enum":["no compression"],"default":"no compression"}}},{"title":"Deflate","required":["codec","compression_level"],"properties":{"codec":{"type":"string","enum":["Deflate"],"default":"Deflate"},"compression_level":{"title":"Deflate Level","description":"0: no compression & fastest, 9: best compression & slowest.","type":"integer","default":0,"minimum":0,"maximum":9}}},{"title":"bzip2","required":["codec"],"properties":{"codec":{"type":"string","enum":["bzip2"],"default":"bzip2"}}},{"title":"xz","required":["codec","compression_level"],"properties":{"codec":{"type":"string","enum":["xz"],"default":"xz"},"compression_level":{"title":"Compression Level","description":"See <a href=\"https://commons.apache.org/proper/commons-compress/apidocs/org/apache/commons/compress/compressors/xz/XZCompressorOutputStream.html#XZCompressorOutputStream-java.io.OutputStream-int-\">here</a> for details.","type":"integer","default":6,"minimum":0,"maximum":9}}},{"title":"zstandard","required":["codec","compression_level"],"properties":{"codec":{"type":"string","enum":["zstandard"],"default":"zstandard"},"compression_level":{"title":"Compression Level","description":"Negative levels are 'fast' modes akin to lz4 or snappy, levels above 9 are generally for archival purposes, and levels above 18 use a lot of memory.","type":"integer","default":3,"minimum":-5,"maximum":22},"include_checksum":{"title":"Include Checksum","description":"If true, include a checksum with each data block.","type":"boolean","default":false}}},{"title":"snappy","required":["codec"],"properties":{"codec":{"type":"string","enum":["snappy"],"default":"snappy"}}}],"order":1}}},{"title":"CSV: Comma-Separated Values","required":["format_type","flattening"],"properties":{"format_type":{"title":"Format Type","type":"string","enum":["CSV"],"default":"CSV"},"flattening":{"type":"string","title":"Flattening","description":"Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details.","default":"No flattening","enum":["No flattening","Root level flattening"]},"compression":{"title":"Compression","type":"object","description":"Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \".csv.gz\").","oneOf":[{"title":"No Compression","requires":["compression_type"],"properties":{"compression_type":{"type":"string","enum":["No Compression"],"default":"No Compression"}}},{"title":"GZIP","requires":["compression_type"],"properties":{"compression_type":{"type":"string","enum":["GZIP"],"default":"GZIP"}}}]}}},{"title":"JSON Lines: Newline-delimited JSON","required":["format_type"],"properties":{"format_type":{"title":"Format Type","type":"string","enum":["JSONL"],"default":"JSONL"},"flattening":{"type":"string","title":"Flattening","description":"Whether the input json data should be normalized (flattened) in the output JSON Lines. Please refer to docs for details.","default":"No flattening","enum":["No flattening","Root level flattening"]},"compression":{"title":"Compression","type":"object","description":"Whether the output files should be compressed. If compression is selected, the output filename will have an extra extension (GZIP: \".jsonl.gz\").","oneOf":[{"title":"No Compression","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["No Compression"],"default":"No Compression"}}},{"title":"GZIP","requires":"compression_type","properties":{"compression_type":{"type":"string","enum":["GZIP"],"default":"GZIP"}}}]}}},{"title":"Parquet: Columnar Storage","required":["format_type"],"properties":{"format_type":{"title":"Format Type","type":"string","enum":["Parquet"],"default":"Parquet"},"compression_codec":{"title":"Compression Codec","description":"The compression algorithm used to compress data pages.","type":"string","enum":["UNCOMPRESSED","SNAPPY","GZIP","LZO","BROTLI","LZ4","ZSTD"],"default":"UNCOMPRESSED"},"block_size_mb":{"title":"Block Size (Row Group Size) (MB)","description":"This is the size of a row group being buffered in memory. It limits the memory usage when writing. Larger values will improve the IO when reading, but consume more memory when writing. Default: 128 MB.","type":"integer","default":128,"examples":[128]},"max_padding_size_mb":{"title":"Max Padding Size (MB)","description":"Maximum size allowed as padding to align row groups. This is also the minimum size of a row group. Default: 8 MB.","type":"integer","default":8,"examples":[8]},"page_size_kb":{"title":"Page Size (KB)","description":"The page size is for compression. A block is composed of pages. A page is the smallest unit that must be read fully to access a single record. If this value is too small, the compression will deteriorate. Default: 1024 KB.","type":"integer","default":1024,"examples":[1024]},"dictionary_page_size_kb":{"title":"Dictionary Page Size (KB)","description":"There is one dictionary page per column per row group when dictionary encoding is used. The dictionary page size works like the page size but for dictionary. Default: 1024 KB.","type":"integer","default":1024,"examples":[1024]},"dictionary_encoding":{"title":"Dictionary Encoding","description":"Default: true.","type":"boolean","default":true}}}],"order":5},"s3_endpoint":{"title":"Endpoint","type":"string","default":"","description":"Your S3 endpoint url. Read more <a href=\"https://docs.aws.amazon.com/general/latest/gr/s3.html#:~:text=Service%20endpoints-,Amazon%20S3%20endpoints,-When%20you%20use\">here</a>","examples":["http://localhost:9000"],"order":6},"s3_path_format":{"title":"S3 Path Format","description":"Format string on how data will be organized inside the S3 bucket directory. Read more <a href=\"https://docs.airbyte.com/integrations/destinations/s3#:~:text=The%20full%20path%20of%20the%20output%20data%20with%20the%20default%20S3%20path%20format\">here</a>","type":"string","examples":["${NAMESPACE}/${STREAM_NAME}/${YEAR}_${MONTH}_${DAY}_${EPOCH}_"],"order":7},"file_name_pattern":{"type":"string","description":"The pattern allows you to set the file-name format for the S3 staging file(s)","title":"S3 Filename pattern","examples":["{date}","{date:yyyy_MM}","{timestamp}","{part_number}","{sync_id}"],"order":8}}},"destination-redis":{"title":"Redis Destination Spec","type":"object","required":["host","username","port","cache_type","destinationType"],"properties":{"host":{"title":"Host","description":"Redis host to connect to.","type":"string","examples":["localhost,127.0.0.1"],"order":1},"port":{"title":"Port","description":"Port of Redis.","type":"integer","minimum":0,"maximum":65536,"default":6379,"order":2},"username":{"title":"Username","description":"Username associated with Redis.","type":"string","order":3},"password":{"title":"Password","description":"Password associated with Redis.","type":"string","airbyte_secret":true,"order":4},"ssl":{"title":"SSL Connection","type":"boolean","description":"Indicates whether SSL encryption protocol will be used to connect to Redis. It is recommended to use SSL connection if possible.","default":false,"order":5},"ssl_mode":{"title":"SSL Modes","description":"SSL connection modes. \n  <li><b>verify-full</b> - This is the most secure mode. Always require encryption and verifies the identity of the source database server","type":"object","order":6,"oneOf":[{"title":"disable","additionalProperties":false,"description":"Disable SSL.","required":["mode"],"properties":{"mode":{"type":"string","const":"disable","enum":["disable"],"default":"disable","order":0}}},{"title":"verify-full","additionalProperties":false,"description":"Verify-full SSL mode.","required":["mode","ca_certificate","client_certificate","client_key"],"properties":{"mode":{"type":"string","const":"verify-full","enum":["verify-full"],"default":"verify-full","order":0},"ca_certificate":{"type":"string","title":"CA Certificate","description":"CA certificate","airbyte_secret":true,"multiline":true,"order":1},"client_certificate":{"type":"string","title":"Client Certificate","description":"Client certificate","airbyte_secret":true,"multiline":true,"order":2},"client_key":{"type":"string","title":"Client Key","description":"Client key","airbyte_secret":true,"multiline":true,"order":3},"client_key_password":{"type":"string","title":"Client key password","description":"Password for keystorage. If you do not add it - the password will be generated automatically.","airbyte_secret":true,"order":4}}}]},"cache_type":{"title":"Cache type","type":"string","default":"hash","description":"Redis cache type to store data in.","enum":["hash"],"order":7},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]},"destinationType":{"title":"redis","const":"redis","enum":["redis"],"order":0,"type":"string"}}},"destination-redis-update":{"title":"Redis Destination Spec","type":"object","required":["host","username","port","cache_type"],"properties":{"host":{"title":"Host","description":"Redis host to connect to.","type":"string","examples":["localhost,127.0.0.1"],"order":1},"port":{"title":"Port","description":"Port of Redis.","type":"integer","minimum":0,"maximum":65536,"default":6379,"order":2},"username":{"title":"Username","description":"Username associated with Redis.","type":"string","order":3},"password":{"title":"Password","description":"Password associated with Redis.","type":"string","airbyte_secret":true,"order":4},"ssl":{"title":"SSL Connection","type":"boolean","description":"Indicates whether SSL encryption protocol will be used to connect to Redis. It is recommended to use SSL connection if possible.","default":false,"order":5},"ssl_mode":{"title":"SSL Modes","description":"SSL connection modes. \n  <li><b>verify-full</b> - This is the most secure mode. Always require encryption and verifies the identity of the source database server","type":"object","order":6,"oneOf":[{"title":"disable","additionalProperties":false,"description":"Disable SSL.","required":["mode"],"properties":{"mode":{"type":"string","const":"disable","enum":["disable"],"default":"disable","order":0}}},{"title":"verify-full","additionalProperties":false,"description":"Verify-full SSL mode.","required":["mode","ca_certificate","client_certificate","client_key"],"properties":{"mode":{"type":"string","const":"verify-full","enum":["verify-full"],"default":"verify-full","order":0},"ca_certificate":{"type":"string","title":"CA Certificate","description":"CA certificate","airbyte_secret":true,"multiline":true,"order":1},"client_certificate":{"type":"string","title":"Client Certificate","description":"Client certificate","airbyte_secret":true,"multiline":true,"order":2},"client_key":{"type":"string","title":"Client Key","description":"Client key","airbyte_secret":true,"multiline":true,"order":3},"client_key_password":{"type":"string","title":"Client key password","description":"Password for keystorage. If you do not add it - the password will be generated automatically.","airbyte_secret":true,"order":4}}}]},"cache_type":{"title":"Cache type","type":"string","default":"hash","description":"Redis cache type to store data in.","enum":["hash"],"order":7},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]}}},"destination-typesense":{"title":"Destination Typesense","type":"object","required":["api_key","host","destinationType"],"properties":{"api_key":{"title":"API Key","type":"string","description":"Typesense API Key","order":0},"host":{"title":"Host","type":"string","description":"Hostname of the Typesense instance without protocol.","order":1},"port":{"title":"Port","type":"string","description":"Port of the Typesense instance. Ex: 8108, 80, 443. Default is 443","order":2},"protocol":{"title":"Protocol","type":"string","description":"Protocol of the Typesense instance. Ex: http or https. Default is https","order":3},"batch_size":{"title":"Batch size","type":"integer","description":"How many documents should be imported together. Default 1000","order":4},"destinationType":{"title":"typesense","const":"typesense","enum":["typesense"],"order":0,"type":"string"}}},"destination-typesense-update":{"title":"Destination Typesense","type":"object","required":["api_key","host"],"properties":{"api_key":{"title":"API Key","type":"string","description":"Typesense API Key","order":0},"host":{"title":"Host","type":"string","description":"Hostname of the Typesense instance without protocol.","order":1},"port":{"title":"Port","type":"string","description":"Port of the Typesense instance. Ex: 8108, 80, 443. Default is 443","order":2},"protocol":{"title":"Protocol","type":"string","description":"Protocol of the Typesense instance. Ex: http or https. Default is https","order":3},"batch_size":{"title":"Batch size","type":"integer","description":"How many documents should be imported together. Default 1000","order":4}}},"destination-bigquery":{"title":"BigQuery Destination Spec","type":"object","required":["project_id","dataset_location","dataset_id","destinationType"],"properties":{"project_id":{"type":"string","description":"The GCP project ID for the project containing the target BigQuery dataset. Read more <a href=\"https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects\">here</a>.","title":"Project ID","order":0},"dataset_location":{"type":"string","description":"The location of the dataset. Warning: Changes made after creation will not be applied. Read more <a href=\"https://cloud.google.com/bigquery/docs/locations\">here</a>.","title":"Dataset Location","order":1,"enum":["US","EU","asia-east1","asia-east2","asia-northeast1","asia-northeast2","asia-northeast3","asia-south1","asia-south2","asia-southeast1","asia-southeast2","australia-southeast1","australia-southeast2","europe-central1","europe-central2","europe-north1","europe-southwest1","europe-west1","europe-west2","europe-west3","europe-west4","europe-west6","europe-west7","europe-west8","europe-west9","me-west1","northamerica-northeast1","northamerica-northeast2","southamerica-east1","southamerica-west1","us-central1","us-east1","us-east2","us-east3","us-east4","us-east5","us-west1","us-west2","us-west3","us-west4"]},"dataset_id":{"type":"string","description":"The default BigQuery Dataset ID that tables are replicated to if the source does not specify a namespace. Read more <a href=\"https://cloud.google.com/bigquery/docs/datasets#create-dataset\">here</a>.","title":"Default Dataset ID","order":2},"loading_method":{"type":"object","title":"Loading Method","description":"Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href=\"https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging\">here</a>.","order":3,"oneOf":[{"title":"Standard Inserts","required":["method"],"properties":{"method":{"type":"string","const":"Standard","enum":["Standard"]}}},{"title":"GCS Staging","required":["method","gcs_bucket_name","gcs_bucket_path","credential"],"properties":{"method":{"type":"string","const":"GCS Staging","order":0,"enum":["GCS Staging"]},"credential":{"title":"Credential","description":"An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href=\"https://cloud.google.com/storage/docs/authentication/hmackeys\">here</a>.","type":"object","order":1,"oneOf":[{"title":"HMAC key","required":["credential_type","hmac_key_access_id","hmac_key_secret"],"properties":{"credential_type":{"type":"string","const":"HMAC_KEY","order":0,"enum":["HMAC_KEY"]},"hmac_key_access_id":{"type":"string","description":"HMAC key access ID. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long.","title":"HMAC Key Access ID","airbyte_secret":true,"examples":["1234567890abcdefghij1234"],"order":1},"hmac_key_secret":{"type":"string","description":"The corresponding secret for the access ID. It is a 40-character base-64 encoded string.","title":"HMAC Key Secret","airbyte_secret":true,"examples":["1234567890abcdefghij1234567890ABCDEFGHIJ"],"order":2}}}]},"gcs_bucket_name":{"title":"GCS Bucket Name","type":"string","description":"The name of the GCS bucket. Read more <a href=\"https://cloud.google.com/storage/docs/naming-buckets\">here</a>.","examples":["airbyte_sync"],"order":2},"gcs_bucket_path":{"title":"GCS Bucket Path","description":"Directory under the GCS bucket where data will be written.","type":"string","examples":["data_sync/test"],"order":3},"keep_files_in_gcs-bucket":{"type":"string","description":"This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default \"Delete all tmp files from GCS\" value is used if not set explicitly.","title":"GCS Tmp Files Afterward Processing","default":"Delete all tmp files from GCS","enum":["Delete all tmp files from GCS","Keep all tmp files in GCS"],"order":4},"file_buffer_count":{"title":"File Buffer Count","type":"integer","minimum":10,"maximum":50,"default":10,"description":"Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects","examples":["10"],"order":5}}}]},"credentials_json":{"type":"string","description":"The contents of the JSON service account key. Check out the <a href=\"https://docs.airbyte.com/integrations/destinations/bigquery#service-account-key\">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.","title":"Service Account Key JSON (Required for cloud, optional for open-source)","airbyte_secret":true,"order":4,"always_show":true},"transformation_priority":{"type":"string","description":"Interactive run type means that the query is executed as soon as possible, and these queries count towards concurrent rate limit and daily limit. Read more about interactive run type <a href=\"https://cloud.google.com/bigquery/docs/running-queries#queries\">here</a>. Batch queries are queued and started as soon as idle resources are available in the BigQuery shared resource pool, which usually occurs within a few minutes. Batch queries don’t count towards your concurrent rate limit. Read more about batch queries <a href=\"https://cloud.google.com/bigquery/docs/running-queries#batch\">here</a>. The default \"interactive\" value is used if not set explicitly.","title":"Transformation Query Run Type","default":"interactive","enum":["interactive","batch"],"order":5},"big_query_client_buffer_size_mb":{"title":"Google BigQuery Client Chunk Size","description":"Google BigQuery client's chunk (buffer) size (MIN=1, MAX = 15) for each table. The size that will be written by a single RPC. Written data will be buffered and only flushed upon reaching this size or closing the channel. The default 15MB value is used if not set explicitly. Read more <a href=\"https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html\">here</a>.","type":"integer","minimum":1,"maximum":15,"default":15,"examples":["15"],"order":6},"raw_data_dataset":{"type":"string","description":"The dataset to write raw tables into","title":"Destinations V2 Raw Table Dataset","order":7},"destinationType":{"title":"bigquery","const":"bigquery","enum":["bigquery"],"order":0,"type":"string"}}},"destination-bigquery-update":{"title":"BigQuery Destination Spec","type":"object","required":["project_id","dataset_location","dataset_id"],"properties":{"project_id":{"type":"string","description":"The GCP project ID for the project containing the target BigQuery dataset. Read more <a href=\"https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects\">here</a>.","title":"Project ID","order":0},"dataset_location":{"type":"string","description":"The location of the dataset. Warning: Changes made after creation will not be applied. Read more <a href=\"https://cloud.google.com/bigquery/docs/locations\">here</a>.","title":"Dataset Location","order":1,"enum":["US","EU","asia-east1","asia-east2","asia-northeast1","asia-northeast2","asia-northeast3","asia-south1","asia-south2","asia-southeast1","asia-southeast2","australia-southeast1","australia-southeast2","europe-central1","europe-central2","europe-north1","europe-southwest1","europe-west1","europe-west2","europe-west3","europe-west4","europe-west6","europe-west7","europe-west8","europe-west9","me-west1","northamerica-northeast1","northamerica-northeast2","southamerica-east1","southamerica-west1","us-central1","us-east1","us-east2","us-east3","us-east4","us-east5","us-west1","us-west2","us-west3","us-west4"]},"dataset_id":{"type":"string","description":"The default BigQuery Dataset ID that tables are replicated to if the source does not specify a namespace. Read more <a href=\"https://cloud.google.com/bigquery/docs/datasets#create-dataset\">here</a>.","title":"Default Dataset ID","order":2},"loading_method":{"type":"object","title":"Loading Method","description":"Loading method used to send select the way data will be uploaded to BigQuery. <br/><b>Standard Inserts</b> - Direct uploading using SQL INSERT statements. This method is extremely inefficient and provided only for quick testing. In almost all cases, you should use staging. <br/><b>GCS Staging</b> - Writes large batches of records to a file, uploads the file to GCS, then uses <b>COPY INTO table</b> to upload the file. Recommended for most workloads for better speed and scalability. Read more about GCS Staging <a href=\"https://docs.airbyte.com/integrations/destinations/bigquery#gcs-staging\">here</a>.","order":3,"oneOf":[{"title":"Standard Inserts","required":["method"],"properties":{"method":{"type":"string","const":"Standard","enum":["Standard"]}}},{"title":"GCS Staging","required":["method","gcs_bucket_name","gcs_bucket_path","credential"],"properties":{"method":{"type":"string","const":"GCS Staging","order":0,"enum":["GCS Staging"]},"credential":{"title":"Credential","description":"An HMAC key is a type of credential and can be associated with a service account or a user account in Cloud Storage. Read more <a href=\"https://cloud.google.com/storage/docs/authentication/hmackeys\">here</a>.","type":"object","order":1,"oneOf":[{"title":"HMAC key","required":["credential_type","hmac_key_access_id","hmac_key_secret"],"properties":{"credential_type":{"type":"string","const":"HMAC_KEY","order":0,"enum":["HMAC_KEY"]},"hmac_key_access_id":{"type":"string","description":"HMAC key access ID. When linked to a service account, this ID is 61 characters long; when linked to a user account, it is 24 characters long.","title":"HMAC Key Access ID","airbyte_secret":true,"examples":["1234567890abcdefghij1234"],"order":1},"hmac_key_secret":{"type":"string","description":"The corresponding secret for the access ID. It is a 40-character base-64 encoded string.","title":"HMAC Key Secret","airbyte_secret":true,"examples":["1234567890abcdefghij1234567890ABCDEFGHIJ"],"order":2}}}]},"gcs_bucket_name":{"title":"GCS Bucket Name","type":"string","description":"The name of the GCS bucket. Read more <a href=\"https://cloud.google.com/storage/docs/naming-buckets\">here</a>.","examples":["airbyte_sync"],"order":2},"gcs_bucket_path":{"title":"GCS Bucket Path","description":"Directory under the GCS bucket where data will be written.","type":"string","examples":["data_sync/test"],"order":3},"keep_files_in_gcs-bucket":{"type":"string","description":"This upload method is supposed to temporary store records in GCS bucket. By this select you can chose if these records should be removed from GCS when migration has finished. The default \"Delete all tmp files from GCS\" value is used if not set explicitly.","title":"GCS Tmp Files Afterward Processing","default":"Delete all tmp files from GCS","enum":["Delete all tmp files from GCS","Keep all tmp files in GCS"],"order":4},"file_buffer_count":{"title":"File Buffer Count","type":"integer","minimum":10,"maximum":50,"default":10,"description":"Number of file buffers allocated for writing data. Increasing this number is beneficial for connections using Change Data Capture (CDC) and up to the number of streams within a connection. Increasing the number of file buffers past the maximum number of streams has deteriorating effects","examples":["10"],"order":5}}}]},"credentials_json":{"type":"string","description":"The contents of the JSON service account key. Check out the <a href=\"https://docs.airbyte.com/integrations/destinations/bigquery#service-account-key\">docs</a> if you need help generating this key. Default credentials will be used if this field is left empty.","title":"Service Account Key JSON (Required for cloud, optional for open-source)","airbyte_secret":true,"order":4,"always_show":true},"transformation_priority":{"type":"string","description":"Interactive run type means that the query is executed as soon as possible, and these queries count towards concurrent rate limit and daily limit. Read more about interactive run type <a href=\"https://cloud.google.com/bigquery/docs/running-queries#queries\">here</a>. Batch queries are queued and started as soon as idle resources are available in the BigQuery shared resource pool, which usually occurs within a few minutes. Batch queries don’t count towards your concurrent rate limit. Read more about batch queries <a href=\"https://cloud.google.com/bigquery/docs/running-queries#batch\">here</a>. The default \"interactive\" value is used if not set explicitly.","title":"Transformation Query Run Type","default":"interactive","enum":["interactive","batch"],"order":5},"big_query_client_buffer_size_mb":{"title":"Google BigQuery Client Chunk Size","description":"Google BigQuery client's chunk (buffer) size (MIN=1, MAX = 15) for each table. The size that will be written by a single RPC. Written data will be buffered and only flushed upon reaching this size or closing the channel. The default 15MB value is used if not set explicitly. Read more <a href=\"https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html\">here</a>.","type":"integer","minimum":1,"maximum":15,"default":15,"examples":["15"],"order":6},"raw_data_dataset":{"type":"string","description":"The dataset to write raw tables into","title":"Destinations V2 Raw Table Dataset","order":7}}},"destination-vertica":{"title":"Vertica Destination Spec","type":"object","required":["host","port","username","database","schema","destinationType"],"properties":{"host":{"title":"Host","description":"Hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"Port of the database.","type":"integer","minimum":0,"maximum":65536,"default":5433,"examples":["5433"],"order":1},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":2},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":3},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":4},"jdbc_url_params":{"description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","title":"JDBC URL Params","type":"string","order":6},"schema":{"title":"Schema","description":"Schema for vertica destination","type":"string","order":7},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]},"destinationType":{"title":"vertica","const":"vertica","enum":["vertica"],"order":0,"type":"string"}}},"destination-vertica-update":{"title":"Vertica Destination Spec","type":"object","required":["host","port","username","database","schema"],"properties":{"host":{"title":"Host","description":"Hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"Port of the database.","type":"integer","minimum":0,"maximum":65536,"default":5433,"examples":["5433"],"order":1},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":2},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":3},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":4},"jdbc_url_params":{"description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","title":"JDBC URL Params","type":"string","order":6},"schema":{"title":"Schema","description":"Schema for vertica destination","type":"string","order":7},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]}}},"destination-elasticsearch":{"title":"Elasticsearch Connection Configuration","type":"object","required":["endpoint","destinationType"],"properties":{"endpoint":{"title":"Server Endpoint","type":"string","description":"The full url of the Elasticsearch server"},"upsert":{"type":"boolean","title":"Upsert Records","description":"If a primary key identifier is defined in the source, an upsert will be performed using the primary key value as the elasticsearch doc id. Does not support composite primary keys.","default":true},"ca_certificate":{"type":"string","title":"CA certificate","description":"CA certificate","airbyte_secret":true,"multiline":true},"authenticationMethod":{"title":"Authentication Method","type":"object","description":"The type of authentication to be used","oneOf":[{"title":"Api Key/Secret","additionalProperties":false,"description":"Use a api key and secret combination to authenticate","required":["method","apiKeyId","apiKeySecret"],"properties":{"method":{"type":"string","const":"secret","enum":["secret"]},"apiKeyId":{"title":"API Key ID","description":"The Key ID to used when accessing an enterprise Elasticsearch instance.","type":"string"},"apiKeySecret":{"title":"API Key Secret","description":"The secret associated with the API Key ID.","type":"string","airbyte_secret":true}}},{"title":"Username/Password","additionalProperties":false,"description":"Basic auth header with a username and password","required":["method","username","password"],"properties":{"method":{"type":"string","const":"basic","enum":["basic"]},"username":{"title":"Username","description":"Basic auth username to access a secure Elasticsearch server","type":"string"},"password":{"title":"Password","description":"Basic auth password to access a secure Elasticsearch server","type":"string","airbyte_secret":true}}}]},"destinationType":{"title":"elasticsearch","const":"elasticsearch","enum":["elasticsearch"],"order":0,"type":"string"}}},"destination-elasticsearch-update":{"title":"Elasticsearch Connection Configuration","type":"object","required":["endpoint"],"properties":{"endpoint":{"title":"Server Endpoint","type":"string","description":"The full url of the Elasticsearch server"},"upsert":{"type":"boolean","title":"Upsert Records","description":"If a primary key identifier is defined in the source, an upsert will be performed using the primary key value as the elasticsearch doc id. Does not support composite primary keys.","default":true},"ca_certificate":{"type":"string","title":"CA certificate","description":"CA certificate","airbyte_secret":true,"multiline":true},"authenticationMethod":{"title":"Authentication Method","type":"object","description":"The type of authentication to be used","oneOf":[{"title":"Api Key/Secret","additionalProperties":false,"description":"Use a api key and secret combination to authenticate","required":["method","apiKeyId","apiKeySecret"],"properties":{"method":{"type":"string","const":"secret","enum":["secret"]},"apiKeyId":{"title":"API Key ID","description":"The Key ID to used when accessing an enterprise Elasticsearch instance.","type":"string"},"apiKeySecret":{"title":"API Key Secret","description":"The secret associated with the API Key ID.","type":"string","airbyte_secret":true}}},{"title":"Username/Password","additionalProperties":false,"description":"Basic auth header with a username and password","required":["method","username","password"],"properties":{"method":{"type":"string","const":"basic","enum":["basic"]},"username":{"title":"Username","description":"Basic auth username to access a secure Elasticsearch server","type":"string"},"password":{"title":"Password","description":"Basic auth password to access a secure Elasticsearch server","type":"string","airbyte_secret":true}}}]}}},"destination-azure-blob-storage":{"title":"AzureBlobStorage Destination Spec","type":"object","required":["azure_blob_storage_account_name","azure_blob_storage_account_key","format","destinationType"],"properties":{"azure_blob_storage_endpoint_domain_name":{"title":"Endpoint Domain Name","type":"string","default":"blob.core.windows.net","description":"This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native from example.","examples":["blob.core.windows.net"]},"azure_blob_storage_container_name":{"title":"Azure blob storage container (Bucket) Name","type":"string","description":"The name of the Azure blob storage container. If not exists - will be created automatically. May be empty, then will be created automatically airbytecontainer+timestamp","examples":["airbytetescontainername"]},"azure_blob_storage_account_name":{"title":"Azure Blob Storage account name","type":"string","description":"The account's name of the Azure Blob Storage.","examples":["airbyte5storage"]},"azure_blob_storage_account_key":{"title":"Azure Blob Storage account key","description":"The Azure blob storage account key.","airbyte_secret":true,"type":"string","examples":["Z8ZkZpteggFx394vm+PJHnGTvdRncaYS+JhLKdj789YNmD+iyGTnG+PV+POiuYNhBg/ACS+LKjd%4FG3FHGN12Nd=="]},"azure_blob_storage_output_buffer_size":{"title":"Azure Blob Storage output buffer size (Megabytes)","type":"integer","description":"The amount of megabytes to buffer for the output stream to Azure. This will impact memory footprint on workers, but may need adjustment for performance and appropriate block size in Azure.","minimum":1,"maximum":2047,"default":5,"examples":[5]},"azure_blob_storage_spill_size":{"title":"Azure Blob Storage file spill size","type":"integer","description":"The amount of megabytes after which the connector should spill the records in a new blob object. Make sure to configure size greater than individual records. Enter 0 if not applicable","default":500,"examples":[500]},"format":{"title":"Output Format","type":"object","description":"Output data format","oneOf":[{"title":"CSV: Comma-Separated Values","required":["format_type","flattening"],"properties":{"format_type":{"type":"string","const":"CSV","enum":["CSV"]},"flattening":{"type":"string","title":"Normalization (Flattening)","description":"Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details.","default":"No flattening","enum":["No flattening","Root level flattening"]}}},{"title":"JSON Lines: newline-delimited JSON","required":["format_type"],"properties":{"format_type":{"type":"string","const":"JSONL","enum":["JSONL"]}}}]},"destinationType":{"title":"azure-blob-storage","const":"azure-blob-storage","enum":["azure-blob-storage"],"order":0,"type":"string"}}},"destination-azure-blob-storage-update":{"title":"AzureBlobStorage Destination Spec","type":"object","required":["azure_blob_storage_account_name","azure_blob_storage_account_key","format"],"properties":{"azure_blob_storage_endpoint_domain_name":{"title":"Endpoint Domain Name","type":"string","default":"blob.core.windows.net","description":"This is Azure Blob Storage endpoint domain name. Leave default value (or leave it empty if run container from command line) to use Microsoft native from example.","examples":["blob.core.windows.net"]},"azure_blob_storage_container_name":{"title":"Azure blob storage container (Bucket) Name","type":"string","description":"The name of the Azure blob storage container. If not exists - will be created automatically. May be empty, then will be created automatically airbytecontainer+timestamp","examples":["airbytetescontainername"]},"azure_blob_storage_account_name":{"title":"Azure Blob Storage account name","type":"string","description":"The account's name of the Azure Blob Storage.","examples":["airbyte5storage"]},"azure_blob_storage_account_key":{"title":"Azure Blob Storage account key","description":"The Azure blob storage account key.","airbyte_secret":true,"type":"string","examples":["Z8ZkZpteggFx394vm+PJHnGTvdRncaYS+JhLKdj789YNmD+iyGTnG+PV+POiuYNhBg/ACS+LKjd%4FG3FHGN12Nd=="]},"azure_blob_storage_output_buffer_size":{"title":"Azure Blob Storage output buffer size (Megabytes)","type":"integer","description":"The amount of megabytes to buffer for the output stream to Azure. This will impact memory footprint on workers, but may need adjustment for performance and appropriate block size in Azure.","minimum":1,"maximum":2047,"default":5,"examples":[5]},"azure_blob_storage_spill_size":{"title":"Azure Blob Storage file spill size","type":"integer","description":"The amount of megabytes after which the connector should spill the records in a new blob object. Make sure to configure size greater than individual records. Enter 0 if not applicable","default":500,"examples":[500]},"format":{"title":"Output Format","type":"object","description":"Output data format","oneOf":[{"title":"CSV: Comma-Separated Values","required":["format_type","flattening"],"properties":{"format_type":{"type":"string","const":"CSV","enum":["CSV"]},"flattening":{"type":"string","title":"Normalization (Flattening)","description":"Whether the input json data should be normalized (flattened) in the output CSV. Please refer to docs for details.","default":"No flattening","enum":["No flattening","Root level flattening"]}}},{"title":"JSON Lines: newline-delimited JSON","required":["format_type"],"properties":{"format_type":{"type":"string","const":"JSONL","enum":["JSONL"]}}}]}}},"destination-langchain":{"title":"Langchain Destination Config","type":"object","properties":{"processing":{"title":"ProcessingConfigModel","type":"object","properties":{"chunk_size":{"title":"Chunk size","description":"Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)","maximum":8191,"type":"integer"},"chunk_overlap":{"title":"Chunk overlap","description":"Size of overlap between chunks in tokens to store in vector store to better capture relevant context","default":0,"type":"integer"},"text_fields":{"title":"Text fields to embed","description":"List of fields in the record that should be used to calculate the embedding. All other fields are passed along as meta fields. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.","always_show":true,"examples":["text","user.name","users.*.name"],"type":"array","items":{"type":"string"}}},"required":["chunk_size","text_fields"],"group":"processing"},"embedding":{"title":"Embedding","description":"Embedding configuration","group":"embedding","type":"object","oneOf":[{"title":"OpenAI","type":"object","properties":{"mode":{"title":"Mode","default":"openai","const":"openai","enum":["openai"],"type":"string"},"openai_key":{"title":"OpenAI API key","airbyte_secret":true,"type":"string"}},"required":["openai_key"],"description":"Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions."},{"title":"Fake","type":"object","properties":{"mode":{"title":"Mode","default":"fake","const":"fake","enum":["fake"],"type":"string"}},"description":"Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs."}]},"indexing":{"title":"Indexing","description":"Indexing configuration","group":"indexing","type":"object","oneOf":[{"title":"Pinecone","type":"object","properties":{"mode":{"title":"Mode","default":"pinecone","const":"pinecone","enum":["pinecone"],"type":"string"},"pinecone_key":{"title":"Pinecone API key","airbyte_secret":true,"type":"string"},"pinecone_environment":{"title":"Pinecone environment","description":"Pinecone environment to use","type":"string"},"index":{"title":"Index","description":"Pinecone index to use","type":"string"}},"required":["pinecone_key","pinecone_environment","index"],"description":"Pinecone is a popular vector store that can be used to store and retrieve embeddings. It is a managed service and can also be queried from outside of langchain."},{"title":"DocArrayHnswSearch","type":"object","properties":{"mode":{"title":"Mode","default":"DocArrayHnswSearch","const":"DocArrayHnswSearch","enum":["DocArrayHnswSearch"],"type":"string"},"destination_path":{"title":"Destination Path","description":"Path to the directory where hnswlib and meta data files will be written. The files will be placed inside that local mount. All files in the specified destination directory will be deleted on each run.","examples":["/local/my_hnswlib_index"],"type":"string"}},"required":["destination_path"],"description":"DocArrayHnswSearch is a lightweight Document Index implementation provided by Docarray that runs fully locally and is best suited for small- to medium-sized datasets. It stores vectors on disk in hnswlib, and stores all other data in SQLite."},{"title":"Chroma (local persistance)","type":"object","properties":{"mode":{"title":"Mode","default":"chroma_local","const":"chroma_local","enum":["chroma_local"],"type":"string"},"destination_path":{"title":"Destination Path","description":"Path to the directory where chroma files will be written. The files will be placed inside that local mount.","examples":["/local/my_chroma_db"],"type":"string"},"collection_name":{"title":"Collection Name","description":"Name of the collection to use.","default":"langchain","type":"string"}},"required":["destination_path"],"description":"Chroma is a popular vector store that can be used to store and retrieve embeddings. It will build its index in memory and persist it to disk by the end of the sync."}]},"destinationType":{"title":"langchain","const":"langchain","enum":["langchain"],"order":0,"type":"string"}},"required":["processing","embedding","indexing","destinationType"],"groups":[{"id":"processing","title":"Processing"},{"id":"embedding","title":"Embedding"},{"id":"indexing","title":"Indexing"}]},"destination-langchain-update":{"title":"Langchain Destination Config","type":"object","properties":{"processing":{"title":"ProcessingConfigModel","type":"object","properties":{"chunk_size":{"title":"Chunk size","description":"Size of chunks in tokens to store in vector store (make sure it is not too big for the context if your LLM)","maximum":8191,"type":"integer"},"chunk_overlap":{"title":"Chunk overlap","description":"Size of overlap between chunks in tokens to store in vector store to better capture relevant context","default":0,"type":"integer"},"text_fields":{"title":"Text fields to embed","description":"List of fields in the record that should be used to calculate the embedding. All other fields are passed along as meta fields. The field list is applied to all streams in the same way and non-existing fields are ignored. If none are defined, all fields are considered text fields. When specifying text fields, you can access nested fields in the record by using dot notation, e.g. `user.name` will access the `name` field in the `user` object. It's also possible to use wildcards to access all fields in an object, e.g. `users.*.name` will access all `names` fields in all entries of the `users` array.","always_show":true,"examples":["text","user.name","users.*.name"],"type":"array","items":{"type":"string"}}},"required":["chunk_size","text_fields"],"group":"processing"},"embedding":{"title":"Embedding","description":"Embedding configuration","group":"embedding","type":"object","oneOf":[{"title":"OpenAI","type":"object","properties":{"mode":{"title":"Mode","default":"openai","const":"openai","enum":["openai"],"type":"string"},"openai_key":{"title":"OpenAI API key","airbyte_secret":true,"type":"string"}},"required":["openai_key"],"description":"Use the OpenAI API to embed text. This option is using the text-embedding-ada-002 model with 1536 embedding dimensions."},{"title":"Fake","type":"object","properties":{"mode":{"title":"Mode","default":"fake","const":"fake","enum":["fake"],"type":"string"}},"description":"Use a fake embedding made out of random vectors with 1536 embedding dimensions. This is useful for testing the data pipeline without incurring any costs."}]},"indexing":{"title":"Indexing","description":"Indexing configuration","group":"indexing","type":"object","oneOf":[{"title":"Pinecone","type":"object","properties":{"mode":{"title":"Mode","default":"pinecone","const":"pinecone","enum":["pinecone"],"type":"string"},"pinecone_key":{"title":"Pinecone API key","airbyte_secret":true,"type":"string"},"pinecone_environment":{"title":"Pinecone environment","description":"Pinecone environment to use","type":"string"},"index":{"title":"Index","description":"Pinecone index to use","type":"string"}},"required":["pinecone_key","pinecone_environment","index"],"description":"Pinecone is a popular vector store that can be used to store and retrieve embeddings. It is a managed service and can also be queried from outside of langchain."},{"title":"DocArrayHnswSearch","type":"object","properties":{"mode":{"title":"Mode","default":"DocArrayHnswSearch","const":"DocArrayHnswSearch","enum":["DocArrayHnswSearch"],"type":"string"},"destination_path":{"title":"Destination Path","description":"Path to the directory where hnswlib and meta data files will be written. The files will be placed inside that local mount. All files in the specified destination directory will be deleted on each run.","examples":["/local/my_hnswlib_index"],"type":"string"}},"required":["destination_path"],"description":"DocArrayHnswSearch is a lightweight Document Index implementation provided by Docarray that runs fully locally and is best suited for small- to medium-sized datasets. It stores vectors on disk in hnswlib, and stores all other data in SQLite."},{"title":"Chroma (local persistance)","type":"object","properties":{"mode":{"title":"Mode","default":"chroma_local","const":"chroma_local","enum":["chroma_local"],"type":"string"},"destination_path":{"title":"Destination Path","description":"Path to the directory where chroma files will be written. The files will be placed inside that local mount.","examples":["/local/my_chroma_db"],"type":"string"},"collection_name":{"title":"Collection Name","description":"Name of the collection to use.","default":"langchain","type":"string"}},"required":["destination_path"],"description":"Chroma is a popular vector store that can be used to store and retrieve embeddings. It will build its index in memory and persist it to disk by the end of the sync."}]}},"required":["processing","embedding","indexing"],"groups":[{"id":"processing","title":"Processing"},{"id":"embedding","title":"Embedding"},{"id":"indexing","title":"Indexing"}]},"destination-cumulio":{"title":"Destination Cumulio","type":"object","required":["api_host","api_key","api_token","destinationType"],"properties":{"api_host":{"title":"Cumul.io API Host URL","description":"URL of the Cumul.io API (e.g. 'https://api.cumul.io', 'https://api.us.cumul.io', or VPC-specific API url). Defaults to 'https://api.cumul.io'.","default":"https://api.cumul.io","type":"string","order":0},"api_key":{"title":"Cumul.io API Key","description":"An API key generated in Cumul.io's platform (can be generated here: https://app.cumul.io/start/profile/integration).","type":"string","airbyte_secret":true,"order":1},"api_token":{"title":"Cumul.io API Token","description":"The corresponding API token generated in Cumul.io's platform (can be generated here: https://app.cumul.io/start/profile/integration).","type":"string","airbyte_secret":true,"order":2},"destinationType":{"title":"cumulio","const":"cumulio","enum":["cumulio"],"order":0,"type":"string"}}},"destination-cumulio-update":{"title":"Destination Cumulio","type":"object","required":["api_host","api_key","api_token"],"properties":{"api_host":{"title":"Cumul.io API Host URL","description":"URL of the Cumul.io API (e.g. 'https://api.cumul.io', 'https://api.us.cumul.io', or VPC-specific API url). Defaults to 'https://api.cumul.io'.","default":"https://api.cumul.io","type":"string","order":0},"api_key":{"title":"Cumul.io API Key","description":"An API key generated in Cumul.io's platform (can be generated here: https://app.cumul.io/start/profile/integration).","type":"string","airbyte_secret":true,"order":1},"api_token":{"title":"Cumul.io API Token","description":"The corresponding API token generated in Cumul.io's platform (can be generated here: https://app.cumul.io/start/profile/integration).","type":"string","airbyte_secret":true,"order":2}}},"destination-postgres":{"title":"Postgres Destination Spec","type":"object","required":["host","port","username","database","schema","destinationType"],"properties":{"host":{"title":"Host","description":"Hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"Port of the database.","type":"integer","minimum":0,"maximum":65536,"default":5432,"examples":["5432"],"order":1},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":2},"schema":{"title":"Default Schema","description":"The default schema tables are written to if the source does not specify a namespace. The usual value for this field is \"public\".","type":"string","examples":["public"],"default":"public","order":3},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":4},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":5},"ssl_mode":{"title":"SSL modes","description":"SSL connection modes. \n <b>disable</b> - Chose this mode to disable encryption of communication between Airbyte and destination database\n <b>allow</b> - Chose this mode to enable encryption only when required by the source database\n <b>prefer</b> - Chose this mode to allow unencrypted connection only if the source database does not support encryption\n <b>require</b> - Chose this mode to always require encryption. If the source database server does not support encryption, connection will fail\n  <b>verify-ca</b> - Chose this mode to always require encryption and to verify that the source database server has a valid SSL certificate\n  <b>verify-full</b> - This is the most secure mode. Chose this mode to always require encryption and to verify the identity of the source database server\n See more information - <a href=\"https://jdbc.postgresql.org/documentation/head/ssl-client.html\"> in the docs</a>.","type":"object","order":7,"oneOf":[{"title":"disable","additionalProperties":false,"description":"Disable SSL.","required":["mode"],"properties":{"mode":{"type":"string","const":"disable","enum":["disable"],"default":"disable","order":0}}},{"title":"allow","additionalProperties":false,"description":"Allow SSL mode.","required":["mode"],"properties":{"mode":{"type":"string","const":"allow","enum":["allow"],"default":"allow","order":0}}},{"title":"prefer","additionalProperties":false,"description":"Prefer SSL mode.","required":["mode"],"properties":{"mode":{"type":"string","const":"prefer","enum":["prefer"],"default":"prefer","order":0}}},{"title":"require","additionalProperties":false,"description":"Require SSL mode.","required":["mode"],"properties":{"mode":{"type":"string","const":"require","enum":["require"],"default":"require","order":0}}},{"title":"verify-ca","additionalProperties":false,"description":"Verify-ca SSL mode.","required":["mode","ca_certificate"],"properties":{"mode":{"type":"string","const":"verify-ca","enum":["verify-ca"],"default":"verify-ca","order":0},"ca_certificate":{"type":"string","title":"CA certificate","description":"CA certificate","airbyte_secret":true,"multiline":true,"order":1},"client_key_password":{"type":"string","title":"Client key password","description":"Password for keystorage. This field is optional. If you do not add it - the password will be generated automatically.","airbyte_secret":true,"order":4}}},{"title":"verify-full","additionalProperties":false,"description":"Verify-full SSL mode.","required":["mode","ca_certificate","client_certificate","client_key"],"properties":{"mode":{"type":"string","const":"verify-full","enum":["verify-full"],"default":"verify-full","order":0},"ca_certificate":{"type":"string","title":"CA certificate","description":"CA certificate","airbyte_secret":true,"multiline":true,"order":1},"client_certificate":{"type":"string","title":"Client certificate","description":"Client certificate","airbyte_secret":true,"multiline":true,"order":2},"client_key":{"type":"string","title":"Client key","description":"Client key","airbyte_secret":true,"multiline":true,"order":3},"client_key_password":{"type":"string","title":"Client key password","description":"Password for keystorage. This field is optional. If you do not add it - the password will be generated automatically.","airbyte_secret":true,"order":4}}}]},"jdbc_url_params":{"description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","title":"JDBC URL Params","type":"string","order":8},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]},"destinationType":{"title":"postgres","const":"postgres","enum":["postgres"],"order":0,"type":"string"}}},"destination-postgres-update":{"title":"Postgres Destination Spec","type":"object","required":["host","port","username","database","schema"],"properties":{"host":{"title":"Host","description":"Hostname of the database.","type":"string","order":0},"port":{"title":"Port","description":"Port of the database.","type":"integer","minimum":0,"maximum":65536,"default":5432,"examples":["5432"],"order":1},"database":{"title":"DB Name","description":"Name of the database.","type":"string","order":2},"schema":{"title":"Default Schema","description":"The default schema tables are written to if the source does not specify a namespace. The usual value for this field is \"public\".","type":"string","examples":["public"],"default":"public","order":3},"username":{"title":"User","description":"Username to use to access the database.","type":"string","order":4},"password":{"title":"Password","description":"Password associated with the username.","type":"string","airbyte_secret":true,"order":5},"ssl_mode":{"title":"SSL modes","description":"SSL connection modes. \n <b>disable</b> - Chose this mode to disable encryption of communication between Airbyte and destination database\n <b>allow</b> - Chose this mode to enable encryption only when required by the source database\n <b>prefer</b> - Chose this mode to allow unencrypted connection only if the source database does not support encryption\n <b>require</b> - Chose this mode to always require encryption. If the source database server does not support encryption, connection will fail\n  <b>verify-ca</b> - Chose this mode to always require encryption and to verify that the source database server has a valid SSL certificate\n  <b>verify-full</b> - This is the most secure mode. Chose this mode to always require encryption and to verify the identity of the source database server\n See more information - <a href=\"https://jdbc.postgresql.org/documentation/head/ssl-client.html\"> in the docs</a>.","type":"object","order":7,"oneOf":[{"title":"disable","additionalProperties":false,"description":"Disable SSL.","required":["mode"],"properties":{"mode":{"type":"string","const":"disable","enum":["disable"],"default":"disable","order":0}}},{"title":"allow","additionalProperties":false,"description":"Allow SSL mode.","required":["mode"],"properties":{"mode":{"type":"string","const":"allow","enum":["allow"],"default":"allow","order":0}}},{"title":"prefer","additionalProperties":false,"description":"Prefer SSL mode.","required":["mode"],"properties":{"mode":{"type":"string","const":"prefer","enum":["prefer"],"default":"prefer","order":0}}},{"title":"require","additionalProperties":false,"description":"Require SSL mode.","required":["mode"],"properties":{"mode":{"type":"string","const":"require","enum":["require"],"default":"require","order":0}}},{"title":"verify-ca","additionalProperties":false,"description":"Verify-ca SSL mode.","required":["mode","ca_certificate"],"properties":{"mode":{"type":"string","const":"verify-ca","enum":["verify-ca"],"default":"verify-ca","order":0},"ca_certificate":{"type":"string","title":"CA certificate","description":"CA certificate","airbyte_secret":true,"multiline":true,"order":1},"client_key_password":{"type":"string","title":"Client key password","description":"Password for keystorage. This field is optional. If you do not add it - the password will be generated automatically.","airbyte_secret":true,"order":4}}},{"title":"verify-full","additionalProperties":false,"description":"Verify-full SSL mode.","required":["mode","ca_certificate","client_certificate","client_key"],"properties":{"mode":{"type":"string","const":"verify-full","enum":["verify-full"],"default":"verify-full","order":0},"ca_certificate":{"type":"string","title":"CA certificate","description":"CA certificate","airbyte_secret":true,"multiline":true,"order":1},"client_certificate":{"type":"string","title":"Client certificate","description":"Client certificate","airbyte_secret":true,"multiline":true,"order":2},"client_key":{"type":"string","title":"Client key","description":"Client key","airbyte_secret":true,"multiline":true,"order":3},"client_key_password":{"type":"string","title":"Client key password","description":"Password for keystorage. This field is optional. If you do not add it - the password will be generated automatically.","airbyte_secret":true,"order":4}}}]},"jdbc_url_params":{"description":"Additional properties to pass to the JDBC URL string when connecting to the database formatted as 'key=value' pairs separated by the symbol '&'. (example: key1=value1&key2=value2&key3=value3).","title":"JDBC URL Params","type":"string","order":8},"tunnel_method":{"type":"object","title":"SSH Tunnel Method","description":"Whether to initiate an SSH tunnel before connecting to the database, and if so, which kind of authentication to use.","oneOf":[{"title":"No Tunnel","required":["tunnel_method"],"properties":{"tunnel_method":{"description":"No ssh tunnel needed to connect to database","type":"string","const":"NO_TUNNEL","order":0,"enum":["NO_TUNNEL"]}}},{"title":"SSH Key Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","ssh_key"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and ssh key","type":"string","const":"SSH_KEY_AUTH","order":0,"enum":["SSH_KEY_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host.","type":"string","order":3},"ssh_key":{"title":"SSH Private Key","description":"OS-level user account ssh key credentials in RSA PEM format ( created with ssh-keygen -t rsa -m PEM -f myuser_rsa )","type":"string","airbyte_secret":true,"multiline":true,"order":4}}},{"title":"Password Authentication","required":["tunnel_method","tunnel_host","tunnel_port","tunnel_user","tunnel_user_password"],"properties":{"tunnel_method":{"description":"Connect through a jump server tunnel host using username and password authentication","type":"string","const":"SSH_PASSWORD_AUTH","order":0,"enum":["SSH_PASSWORD_AUTH"]},"tunnel_host":{"title":"SSH Tunnel Jump Server Host","description":"Hostname of the jump server host that allows inbound ssh tunnel.","type":"string","order":1},"tunnel_port":{"title":"SSH Connection Port","description":"Port on the proxy/jump server that accepts inbound ssh connections.","type":"integer","minimum":0,"maximum":65536,"default":22,"examples":["22"],"order":2},"tunnel_user":{"title":"SSH Login Username","description":"OS-level username for logging into the jump server host","type":"string","order":3},"tunnel_user_password":{"title":"Password","description":"OS-level password for logging into the jump server host","type":"string","airbyte_secret":true,"order":4}}}]}}},"DestinationConfiguration":{"description":"The values required to configure the destination.","example":{"user":"charles"},"oneOf":[{"title":"destination-aws-datalake","$ref":"#/components/schemas/destination-aws-datalake"},{"title":"destination-azure-blob-storage","$ref":"#/components/schemas/destination-azure-blob-storage"},{"title":"destination-bigquery","$ref":"#/components/schemas/destination-bigquery"},{"title":"destination-bigquery-denormalized","$ref":"#/components/schemas/destination-bigquery-denormalized"},{"title":"destination-clickhouse","$ref":"#/components/schemas/destination-clickhouse"},{"title":"destination-convex","$ref":"#/components/schemas/destination-convex"},{"title":"destination-cumulio","$ref":"#/components/schemas/destination-cumulio"},{"title":"destination-databend","$ref":"#/components/schemas/destination-databend"},{"title":"destination-databricks","$ref":"#/components/schemas/destination-databricks"},{"title":"destination-dev-null","$ref":"#/components/schemas/destination-dev-null"},{"title":"destination-dynamodb","$ref":"#/components/schemas/destination-dynamodb"},{"title":"destination-elasticsearch","$ref":"#/components/schemas/destination-elasticsearch"},{"title":"destination-firebolt","$ref":"#/components/schemas/destination-firebolt"},{"title":"destination-firestore","$ref":"#/components/schemas/destination-firestore"},{"title":"destination-gcs","$ref":"#/components/schemas/destination-gcs"},{"title":"destination-google-sheets","$ref":"#/components/schemas/destination-google-sheets"},{"title":"destination-keen","$ref":"#/components/schemas/destination-keen"},{"title":"destination-kinesis","$ref":"#/components/schemas/destination-kinesis"},{"title":"destination-langchain","$ref":"#/components/schemas/destination-langchain"},{"title":"destination-milvus","$ref":"#/components/schemas/destination-milvus"},{"title":"destination-mongodb","$ref":"#/components/schemas/destination-mongodb"},{"title":"destination-mssql","$ref":"#/components/schemas/destination-mssql"},{"title":"destination-mysql","$ref":"#/components/schemas/destination-mysql"},{"title":"destination-oracle","$ref":"#/components/schemas/destination-oracle"},{"title":"destination-pinecone","$ref":"#/components/schemas/destination-pinecone"},{"title":"destination-postgres","$ref":"#/components/schemas/destination-postgres"},{"title":"destination-pubsub","$ref":"#/components/schemas/destination-pubsub"},{"title":"destination-redis","$ref":"#/components/schemas/destination-redis"},{"title":"destination-redshift","$ref":"#/components/schemas/destination-redshift"},{"title":"destination-s3","$ref":"#/components/schemas/destination-s3"},{"title":"destination-s3-glue","$ref":"#/components/schemas/destination-s3-glue"},{"title":"destination-sftp-json","$ref":"#/components/schemas/destination-sftp-json"},{"title":"destination-snowflake","$ref":"#/components/schemas/destination-snowflake"},{"title":"destination-timeplus","$ref":"#/components/schemas/destination-timeplus"},{"title":"destination-typesense","$ref":"#/components/schemas/destination-typesense"},{"title":"destination-vertica","$ref":"#/components/schemas/destination-vertica"},{"title":"destination-xata","$ref":"#/components/schemas/destination-xata"}]},"SourceConfiguration":{"description":"The values required to configure the source.","example":{"user":"charles"}}},"securitySchemes":{"bearerAuth":{"type":"http","scheme":"bearer","bearerFormat":"JWT"}}},"security":[{"bearerAuth":[]}]}